{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Third_Copy_of_BERTForSubclassResearchPaper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js1Lt7xzp53p"
      },
      "source": [
        "# Using BERT-Base-Uncased for D-FJ and FactJudge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZ0kz1wqEk2"
      },
      "source": [
        "### Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rmv3qUboPtC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "from tqdm import tqdm, trange\n",
        "import pickle\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2RtnGzFp4cb"
      },
      "source": [
        "### Checking if GPU available\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaF0biAJqSVc",
        "outputId": "69ada70f-7d0b-421d-b8d3-aed9416ffa6f"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'{torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVT6GHPmqvhM"
      },
      "source": [
        "### Downloading and Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmY3EHwDsyht",
        "outputId": "543655e2-9fdb-4cf1-d7ad-8cfae21647ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "57Fml-XJca5a",
        "outputId": "3195ed29-afae-48d3-e1fb-d9bbce1ba5d9"
      },
      "source": [
        "sentences_df = pd.read_csv(\"/content/gdrive/My Drive/SentencesLabeling/labeled_sentences.csv\")\n",
        "sentences_df = sentences_df[sentences_df[\"Label\"] == 1]\n",
        "sentences_df = sentences_df.dropna()\n",
        "sentences_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "      <th>Subcat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We have investigated the electronic structure ...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It was found that the Sn valence states (5s, 5...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It was demonstrated that the metallic states a...</td>\n",
              "      <td>1</td>\n",
              "      <td>MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>34 11\\nv1 [\\nco nd\\n-m at\\n.m tr\\nlsc\\ni] 1\\n8...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Finkelstein∗, A.Moewes+\\n+Department of Physic...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It was found that the Sn valence states (5s, 5...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>It was demonstrated that the metallic states a...</td>\n",
              "      <td>1</td>\n",
              "      <td>MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PACS: 74.20.Pq, 74.70.Ad, 74.62.Fj, 78.70.En\\n...</td>\n",
              "      <td>1</td>\n",
              "      <td>PARAMETER|METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>It was found that electronic structure of SnO ...</td>\n",
              "      <td>1</td>\n",
              "      <td>MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>both the crystal and electronic structure are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>PARAMETER|MATERIAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Sentence  ...                     Subcat\n",
              "0   We have investigated the electronic structure ...  ...            METHOD|MATERIAL\n",
              "1   It was found that the Sn valence states (5s, 5...  ...                     METHOD\n",
              "2   It was demonstrated that the metallic states a...  ...                   MATERIAL\n",
              "5   34 11\\nv1 [\\nco nd\\n-m at\\n.m tr\\nlsc\\ni] 1\\n8...  ...            METHOD|MATERIAL\n",
              "7   Finkelstein∗, A.Moewes+\\n+Department of Physic...  ...            METHOD|MATERIAL\n",
              "8   It was found that the Sn valence states (5s, 5...  ...                     METHOD\n",
              "9   It was demonstrated that the metallic states a...  ...                   MATERIAL\n",
              "11  PACS: 74.20.Pq, 74.70.Ad, 74.62.Fj, 78.70.En\\n...  ...  PARAMETER|METHOD|MATERIAL\n",
              "12  It was found that electronic structure of SnO ...  ...                   MATERIAL\n",
              "13  both the crystal and electronic structure are ...  ...         PARAMETER|MATERIAL\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40uCzfV9ujse"
      },
      "source": [
        "### Importing Model Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rjRgryxunRm",
        "outputId": "aa80faf2-ccfc-40e7-f5d9-132f570ec017"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Q_t9WKacvU"
      },
      "source": [
        "### Importing sklearn requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqBl6bh8ab5U"
      },
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, precision_recall_curve, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cbMNLDS0sOH",
        "outputId": "cdc9c1d6-df9c-41a1-c5b4-1bb643785f0e"
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([['MATERIAL', 'METHOD', 'CODE', 'PARAMETER', 'STRUCTURE']])\n",
        "print(f\"classes = {mlb.classes_}\")\n",
        "labels = sentences_df[\"Subcat\"]\n",
        "labels = mlb.transform([label.split(\"|\") for label in labels])\n",
        "labels[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classes = ['CODE' 'MATERIAL' 'METHOD' 'PARAMETER' 'STRUCTURE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 1, 1, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkVYOihYuaOq"
      },
      "source": [
        "### BERT Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqMXPBluY9n"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "def preprocess_bert(data):\n",
        "  encoded = tokenizer.encode_plus(\n",
        "      text = data,\n",
        "      add_special_tokens = True,\n",
        "      max_length = MAX_LEN,\n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "  return encoded.get('input_ids'), encoded.get('attention_mask')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqZ-gF4kBHU2"
      },
      "source": [
        "# all_sentences = np.array(hashtags_df['Sentence'])\n",
        "# encoded_sents = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]\n",
        "# max_len = max([len(sent) for sent in encoded_sents])\n",
        "# print('Max length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW5GiVatkeNt",
        "outputId": "d6b19437-a864-4f6a-bef2-44ad6c8e1d9a"
      },
      "source": [
        "MAX_LEN = 128\n",
        "sentences_df['input_ids'], sentences_df['attention_mask'] = zip(*sentences_df['Sentence'].apply(preprocess_bert))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgwMxYsfbfzv"
      },
      "source": [
        "### Fixed Random State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QbfXSlw7mSE"
      },
      "source": [
        "RANDOM_STATE = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-5SGYfbbjwI"
      },
      "source": [
        "### Creating the final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKkHgO4G_I0H",
        "outputId": "a2d07543-b67b-4c4a-f98d-9d91f9c11f15"
      },
      "source": [
        "%%time\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, freeze_bert=False, RNNLayer='RNN', activation='Sigmoid', hidden_nodes=50):\n",
        "    super(BertClassifier, self).__init__()\n",
        "\n",
        "    D_in, H, D_out = 768, hidden_nodes, 5\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    if RNNLayer == 'RNN':\n",
        "      self.rnn = nn.RNN(D_in, H, 2, batch_first = True)\n",
        "    elif RNNLayer == 'LSTM':\n",
        "      self.rnn = nn.LSTM(D_in, H, 2, batch_first = True)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.linear = nn.Linear(H, D_out)\n",
        "    if activation == 'Sigmoid':\n",
        "      self.activation = nn.Sigmoid()\n",
        "    elif activation == 'ReLU':\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == 'Tanh':\n",
        "      self.activation = nn.Tanh()\n",
        "\n",
        "    modules = [self.bert.embeddings, *self.bert.encoder.layer[:2]]\n",
        "    if freeze_bert:\n",
        "      for module in modules:\n",
        "        for param in module.parameters():\n",
        "          param.requires_grad = False\n",
        "    # if freeze_bert:\n",
        "    #   for param in self.bert.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "  \n",
        "  def forward(self, input_ids, attention_masks):\n",
        "\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_masks)\n",
        "    last_hidden_state = outputs[0]\n",
        "    rnn_out, _ = self.rnn(last_hidden_state)\n",
        "    # activated = self.activation()\n",
        "    logits = self.linear(rnn_out[:, -1, :])\n",
        "    # logits = self.linear(last_hidden_state)\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 38 µs, sys: 0 ns, total: 38 µs\n",
            "Wall time: 43.6 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur67S-S2buGf"
      },
      "source": [
        "### Train Test k-fold splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU57AIi5GRmS"
      },
      "source": [
        "def KFoldIds(X_train, labels, k=5):\n",
        "  kf = KFold(n_splits=k, random_state=RANDOM_STATE, shuffle=True)\n",
        "\n",
        "  train_test_splits = []\n",
        "  for train_index, test_index in kf.split(X_train): # splitting in ratio of documents\n",
        "    # arrays containing arrays of sentences in each document\n",
        "    train_test_splits.append((train_index, test_index))\n",
        "  \n",
        "  return train_test_splits\n",
        "\n",
        "def create_train_test_datasets(input_ids, masks, labels, k=5):\n",
        "  input_ids_final = []\n",
        "  masks_final = []\n",
        "  labels_final = []\n",
        "  input_ids_final_test = []\n",
        "  masks_final_test = []\n",
        "  labels_final_test = []\n",
        "\n",
        "  train_test_splits = KFoldIds(input_ids, labels, k)\n",
        "  for train_index, test_index in train_test_splits:\n",
        "    input_ids_tr, input_ids_test = input_ids[train_index], input_ids[test_index]\n",
        "    masks_tr, masks_test = masks[train_index], masks[test_index]\n",
        "    labels_tr, labels_test = labels[train_index], labels[test_index]\n",
        "    \n",
        "    input_ids_tensor = torch.tensor(list(input_ids_tr))\n",
        "    masks_tensor = torch.tensor(list(masks_tr))\n",
        "    labels_tensor = torch.tensor(list(labels_tr), dtype=torch.float)\n",
        "    input_ids_tensor_test = torch.tensor(list(input_ids_test))\n",
        "    masks_tensor_test = torch.tensor(list(masks_test))\n",
        "    labels_tensor_test = torch.tensor(list(labels_test), dtype=torch.float)\n",
        "    # store all the divided parts for reusing again and again\n",
        "    input_ids_final.append(input_ids_tensor)\n",
        "    masks_final.append(masks_tensor)\n",
        "    labels_final.append(labels_tensor)\n",
        "    input_ids_final_test.append(input_ids_tensor_test)\n",
        "    masks_final_test.append(masks_tensor_test)\n",
        "    labels_final_test.append(labels_tensor_test)\n",
        "  \n",
        "  return input_ids_final, masks_final, labels_final, input_ids_final_test, masks_final_test, labels_final_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRVjGpdEb7_9"
      },
      "source": [
        "## Model initializer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXuJg7QTwr6K"
      },
      "source": [
        "def initialize_model(RNNLayer='RNN', activation='Sigmoid'):\n",
        "  # Instantiate Bert Classifier\n",
        "  bert_classifier = BertClassifier(freeze_bert=False, RNNLayer=RNNLayer, activation=activation)\n",
        "\n",
        "  # Tell PyTorch to run the model on GPU\n",
        "  bert_classifier.to(device)\n",
        "\n",
        "  # Create the optimizer\n",
        "  optimizer = AdamW(bert_classifier.parameters(),\n",
        "                    lr=5e-5,    \n",
        "                    eps=1e-8   \n",
        "                    )\n",
        "  \n",
        "  return bert_classifier, optimizer\n",
        "\n",
        "def get_scheduler(train_dataloader, optimizer, epochs=4):\n",
        "  # Total number of training steps\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Set up the learning rate scheduler\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps=0, \n",
        "                                              num_training_steps=total_steps)\n",
        "  return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUZC4Go6cF0X"
      },
      "source": [
        "### Training method for a combination of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISs6NEeK19dl",
        "outputId": "be13ac0d-7921-4601-e44c-865531f01ce7"
      },
      "source": [
        "%%time\n",
        "\n",
        "def train(model, train_dataloader, optimizer, scheduler, test_inputs, test_masks, test_labels, fold, data, epochs=4):\n",
        "  print(\"Start training...\")\n",
        "  # loss_fn = nn.BCELoss()\n",
        "  loss_fn=nn.BCEWithLogitsLoss()\n",
        "  for epoch_i in range(epochs):\n",
        "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^12} | {'Elapsed':^9}\")\n",
        "    print('-'*50)\n",
        "\n",
        "    t0_epoch, t0_batch = time.time(), time.time()\n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts +=1\n",
        "      # Load batch to GPU\n",
        "      b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "      # Zero out any previously calculated gradients\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Perform a forward pass. This will return logits.\n",
        "      logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "      # Compute loss and accumulate the loss values\n",
        "      # loss = loss_fn(logits, b_labels.to(torch.float32).unsqueeze(1))\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters and the learning rate\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "        # Calculate time elapsed for 20 batches\n",
        "        time_elapsed = time.time() - t0_batch\n",
        "\n",
        "        # Print training results\n",
        "        print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "        # Reset batch tracking variables\n",
        "        batch_loss, batch_counts = 0, 0\n",
        "        t0_batch = time.time()\n",
        "        \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Average Train Loss: {avg_train_loss}')\n",
        "\n",
        "    precision, recall, accuracy, f1, precisions, recalls, accuracies, f1s \\\n",
        "      = model_eval(model, test_inputs, test_masks, test_labels)\n",
        "    \n",
        "    \n",
        "    print(f\"{'Epoch':^7} | {'Fold':^5} | {'Data':^6} | {'Precision':^12} | {'Recall':^12} | {'Accuracy':^12} \")\n",
        "    print(f\"{epoch_i+1:^7} | {fold:^5} | {data:^6} | {precision:^7.5f} | {recall:^7.5f} | {accuracy:^7.5f} \")\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.87 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5w0SIXZcOkn"
      },
      "source": [
        "### Predict with trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxRj83YMMHk"
      },
      "source": [
        "def bert_predict(model, input_ids, masks, labels):\n",
        "  model.eval()\n",
        "\n",
        "  test_data = TensorDataset(input_ids, masks, labels)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "  all_probs=[]\n",
        "  for batch in test_dataloader:\n",
        "    # Load batch to GPU\n",
        "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "    # Compute logits\n",
        "    with torch.no_grad():\n",
        "      probs = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "    all_probs.append(probs)\n",
        "\n",
        "  return all_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGzFmmGZWxJo"
      },
      "source": [
        "### Seed Setter method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjiFyjFfVu45"
      },
      "source": [
        "def set_seed(seed_value=42):\n",
        "  \"\"\"Set seed for reproducibility.\n",
        "  \"\"\"\n",
        "  random.seed(seed_value)\n",
        "  np.random.seed(seed_value)\n",
        "  torch.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK1VTlh7qXJh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugyWWbVVqWUe"
      },
      "source": [
        "def model_eval(model, input_ids, masks, labels):\n",
        "  # predicting on mqpa test set\n",
        "  all_probs = bert_predict(model, \n",
        "                                input_ids, \n",
        "                                masks, \n",
        "                                labels)\n",
        "  all_probs = torch.sigmoid(torch.cat(all_probs)).cpu().numpy()\n",
        "  # optimizing prec and rec\n",
        "  # precision, recall, thresholds = precision_recall_curve(np.concatenate(labels).astype('int64'), all_probs[:, 1])\n",
        "  # fscore = (2 * precision * recall) / (precision + recall)\n",
        "  # ix = np.nanargmax(fscore)\n",
        "  preds = np.where(all_probs >= 0.5, 1, 0)\n",
        "  precision = precision_score(labels, preds, average='macro', zero_division=1)\n",
        "  recall = recall_score(labels, preds, average='macro', zero_division=1)\n",
        "  f1 = f1_score(labels, preds, average='macro', zero_division=1)\n",
        "  accuracy = accuracy_score(labels, preds)\n",
        "  precisions = [precision_score(labels[:, i], preds[:, i], zero_division=1) for i in range(5)]\n",
        "  recalls = [recall_score(labels[:, i], preds[:, i], zero_division=1) for i in range(5)]\n",
        "  f1s = [f1_score(labels[:, i], preds[:, i], zero_division=1) for i in range(5)]\n",
        "  accuracies = [accuracy_score(labels[:, i], preds[:, i]) for i in range(5)]\n",
        "  # Testing for P@k and R@k\n",
        "  # prec_at_3, rec_at_3, prec_at_5, rec_at_5 = prec_and_rec(model, input_ids, masks, labels)\n",
        "  \n",
        "  return precision, recall, accuracy, f1, precisions, recalls, accuracies, f1s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEYKg87UdDYM"
      },
      "source": [
        "## The Training Loop with all hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVL6nQuOYwFF"
      },
      "source": [
        "def train_loop():\n",
        "  \n",
        "  k = 5\n",
        "\n",
        "  sentences_input_ids_final, sentences_masks_final, sentences_labels_final, \\\n",
        "    sentences_input_ids_final_test, sentences_masks_final_test, sentences_labels_final_test = \\\n",
        "    create_train_test_datasets(np.array(sentences_df['input_ids']), \n",
        "                               np.array(sentences_df['attention_mask']), \n",
        "                               labels,\n",
        "                               k=k)\n",
        "  RNNLayers = ['RNN']\n",
        "  activations = ['Sigmoid']\n",
        "  batch_sizes = [64]\n",
        "  epochs_s = [10]\n",
        "\n",
        "  for RNNLayer, activation, batch_size, epochs in list(itertools.product(RNNLayers, activations, batch_sizes, epochs_s)):\n",
        "\n",
        "    time0 = time.time()\n",
        "\n",
        "    print(f\"{'Model':^7} | {'Activation':^15} {'Batch Size':^15} | {'Epochs':^10}\")\n",
        "    print(f\"{RNNLayer:^7} | {activation:^15} {batch_size:^15} | {epochs:^10}\")\n",
        "\n",
        "    for i in range(k):\n",
        "      print(f\"Cross-Validation-Batch: {i+1}\")\n",
        "\n",
        "      # training on mqpa\n",
        "      bert_classifier, optimizer = initialize_model(RNNLayer=RNNLayer, activation=activation)\n",
        "\n",
        "      input_ids_final = sentences_input_ids_final[i]\n",
        "      masks_final = sentences_masks_final[i]\n",
        "      labels_final = sentences_labels_final[i]\n",
        "      train_data = TensorDataset(input_ids_final, masks_final, labels_final)\n",
        "      train_sampler = RandomSampler(train_data)\n",
        "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "      scheduler = get_scheduler(train_dataloader, optimizer, epochs=epochs)\n",
        "      model = train(bert_classifier, train_dataloader, optimizer, scheduler, \n",
        "                   sentences_input_ids_final_test[i], sentences_masks_final_test[i], sentences_labels_final_test[i],\n",
        "                    i+1, 'ResearchPapers', epochs=epochs)\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    train_time = str(datetime.timedelta(seconds=time_end-time0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDcLFRCvdKFE",
        "outputId": "d1c7e29e-47ca-40ce-9eb5-ba74e82c0cc1"
      },
      "source": [
        "set_seed(42)\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "train_loop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Model  |   Activation      Batch Size    |   Epochs  \n",
            "  RNN   |     Sigmoid           64        |     10    \n",
            "Cross-Validation-Batch: 1\n",
            "Start training...\n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   20    |   0.531914   |   28.49  \n",
            "   1    |   40    |   0.497757   |   28.71  \n",
            "   1    |   60    |   0.479257   |   28.11  \n",
            "   1    |   80    |   0.456616   |   27.55  \n",
            "   1    |   100   |   0.442412   |   28.11  \n",
            "   1    |   120   |   0.417927   |   28.04  \n",
            "   1    |   140   |   0.389518   |   27.68  \n",
            "   1    |   160   |   0.366037   |   28.03  \n",
            "   1    |   178   |   0.348646   |   24.64  \n",
            "Average Train Loss: 0.43819166178809865\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   1    |   1   | ResearchPapers | 0.96579 | 0.69664 | 0.84325 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   2    |   20    |   0.333118   |   29.37  \n",
            "   2    |   40    |   0.318903   |   28.05  \n",
            "   2    |   60    |   0.313241   |   28.01  \n",
            "   2    |   80    |   0.300997   |   27.97  \n",
            "   2    |   100   |   0.291895   |   27.88  \n",
            "   2    |   120   |   0.283690   |   27.83  \n",
            "   2    |   140   |   0.275756   |   27.80  \n",
            "   2    |   160   |   0.264798   |   27.84  \n",
            "   2    |   178   |   0.268297   |   24.43  \n",
            "Average Train Loss: 0.29503023757614905\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   2    |   1   | ResearchPapers | 0.95453 | 0.94724 | 0.93877 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   3    |   20    |   0.259228   |   29.13  \n",
            "   3    |   40    |   0.250565   |   27.88  \n",
            "   3    |   60    |   0.244962   |   27.84  \n",
            "   3    |   80    |   0.237804   |   27.79  \n",
            "   3    |   100   |   0.238768   |   27.85  \n",
            "   3    |   120   |   0.234075   |   27.91  \n",
            "   3    |   140   |   0.230618   |   27.91  \n",
            "   3    |   160   |   0.227806   |   27.84  \n",
            "   3    |   178   |   0.221505   |   24.43  \n",
            "Average Train Loss: 0.23867510841878434\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   3    |   1   | ResearchPapers | 0.95269 | 0.97149 | 0.94577 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   4    |   20    |   0.216490   |   29.16  \n",
            "   4    |   40    |   0.213743   |   27.94  \n",
            "   4    |   60    |   0.211361   |   27.86  \n",
            "   4    |   80    |   0.212540   |   27.80  \n",
            "   4    |   100   |   0.205011   |   27.73  \n",
            "   4    |   120   |   0.204019   |   27.80  \n",
            "   4    |   140   |   0.194955   |   27.73  \n",
            "   4    |   160   |   0.200379   |   27.89  \n",
            "   4    |   178   |   0.192693   |   24.54  \n",
            "Average Train Loss: 0.20589356074453066\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   4    |   1   | ResearchPapers | 0.96758 | 0.98513 | 0.95766 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   5    |   20    |   0.189182   |   29.31  \n",
            "   5    |   40    |   0.186636   |   27.83  \n",
            "   5    |   60    |   0.188099   |   27.79  \n",
            "   5    |   80    |   0.190522   |   27.77  \n",
            "   5    |   100   |   0.183825   |   27.83  \n",
            "   5    |   120   |   0.186610   |   27.85  \n",
            "   5    |   140   |   0.185632   |   27.92  \n",
            "   5    |   160   |   0.180054   |   27.91  \n",
            "   5    |   178   |   0.176414   |   24.48  \n",
            "Average Train Loss: 0.1853399367472313\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   5    |   1   | ResearchPapers | 0.98118 | 0.96812 | 0.96466 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   6    |   20    |   0.178115   |   29.14  \n",
            "   6    |   40    |   0.170869   |   27.76  \n",
            "   6    |   60    |   0.168344   |   27.79  \n",
            "   6    |   80    |   0.172476   |   27.76  \n",
            "   6    |   100   |   0.171331   |   27.84  \n",
            "   6    |   120   |   0.167720   |   27.88  \n",
            "   6    |   140   |   0.167495   |   27.91  \n",
            "   6    |   160   |   0.165044   |   27.80  \n",
            "   6    |   178   |   0.168244   |   24.44  \n",
            "Average Train Loss: 0.17002452668530982\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   6    |   1   | ResearchPapers | 0.97540 | 0.98447 | 0.97026 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   7    |   20    |   0.163318   |   29.13  \n",
            "   7    |   40    |   0.162147   |   27.80  \n",
            "   7    |   60    |   0.161227   |   27.88  \n",
            "   7    |   80    |   0.163055   |   27.90  \n",
            "   7    |   100   |   0.159463   |   27.86  \n",
            "   7    |   120   |   0.153018   |   27.82  \n",
            "   7    |   140   |   0.155847   |   27.78  \n",
            "   7    |   160   |   0.155450   |   27.82  \n",
            "   7    |   178   |   0.156740   |   24.42  \n",
            "Average Train Loss: 0.15896720110371126\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   7    |   1   | ResearchPapers | 0.98650 | 0.97609 | 0.96921 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   8    |   20    |   0.158461   |   29.12  \n",
            "   8    |   40    |   0.153186   |   27.86  \n",
            "   8    |   60    |   0.152878   |   27.97  \n",
            "   8    |   80    |   0.152168   |   27.86  \n",
            "   8    |   100   |   0.145955   |   27.83  \n",
            "   8    |   120   |   0.152517   |   27.79  \n",
            "   8    |   140   |   0.151117   |   27.75  \n",
            "   8    |   160   |   0.148905   |   27.79  \n",
            "   8    |   178   |   0.148326   |   24.48  \n",
            "Average Train Loss: 0.1515758244851448\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   8    |   1   | ResearchPapers | 0.98335 | 0.98673 | 0.97411 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   9    |   20    |   0.148676   |   29.36  \n",
            "   9    |   40    |   0.149570   |   27.90  \n",
            "   9    |   60    |   0.144819   |   27.87  \n",
            "   9    |   80    |   0.148111   |   27.86  \n",
            "   9    |   100   |   0.145687   |   27.88  \n",
            "   9    |   120   |   0.144753   |   27.84  \n",
            "   9    |   140   |   0.146139   |   27.83  \n",
            "   9    |   160   |   0.146446   |   27.80  \n",
            "   9    |   178   |   0.148310   |   24.50  \n",
            "Average Train Loss: 0.14694012143758423\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   9    |   1   | ResearchPapers | 0.98180 | 0.98586 | 0.97341 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "  10    |   20    |   0.147272   |   29.31  \n",
            "  10    |   40    |   0.145057   |   27.92  \n",
            "  10    |   60    |   0.144069   |   27.89  \n",
            "  10    |   80    |   0.143854   |   27.76  \n",
            "  10    |   100   |   0.141683   |   27.80  \n",
            "  10    |   120   |   0.142962   |   27.94  \n",
            "  10    |   140   |   0.140437   |   27.94  \n",
            "  10    |   160   |   0.146276   |   27.99  \n",
            "  10    |   178   |   0.146973   |   24.55  \n",
            "Average Train Loss: 0.144273620244511\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "  10    |   1   | ResearchPapers | 0.98157 | 0.98650 | 0.97306 \n",
            "Cross-Validation-Batch: 2\n",
            "Start training...\n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   20    |   0.546976   |   29.56  \n",
            "   1    |   40    |   0.507166   |   28.07  \n",
            "   1    |   60    |   0.478214   |   28.04  \n",
            "   1    |   80    |   0.449989   |   28.04  \n",
            "   1    |   100   |   0.423590   |   28.06  \n",
            "   1    |   120   |   0.398982   |   28.04  \n",
            "   1    |   140   |   0.381254   |   27.98  \n",
            "   1    |   160   |   0.364790   |   28.03  \n",
            "   1    |   178   |   0.342861   |   24.64  \n",
            "Average Train Loss: 0.4342888939980022\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   1    |   2   | ResearchPapers | 0.95403 | 0.72318 | 0.86809 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   2    |   20    |   0.334352   |   29.36  \n",
            "   2    |   40    |   0.315529   |   27.97  \n",
            "   2    |   60    |   0.303813   |   27.96  \n",
            "   2    |   80    |   0.296397   |   28.01  \n",
            "   2    |   100   |   0.289439   |   28.01  \n",
            "   2    |   120   |   0.275975   |   28.00  \n",
            "   2    |   140   |   0.270183   |   27.95  \n",
            "   2    |   160   |   0.264959   |   27.83  \n",
            "   2    |   178   |   0.257220   |   24.47  \n",
            "Average Train Loss: 0.2903759354652639\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   2    |   2   | ResearchPapers | 0.97362 | 0.92610 | 0.94717 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   3    |   20    |   0.251962   |   29.25  \n",
            "   3    |   40    |   0.244011   |   27.92  \n",
            "   3    |   60    |   0.238778   |   27.92  \n",
            "   3    |   80    |   0.236042   |   27.87  \n",
            "   3    |   100   |   0.229735   |   27.81  \n",
            "   3    |   120   |   0.225885   |   27.83  \n",
            "   3    |   140   |   0.216706   |   27.94  \n",
            "   3    |   160   |   0.217938   |   27.94  \n",
            "   3    |   178   |   0.210351   |   24.59  \n",
            "Average Train Loss: 0.23049959498743772\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   3    |   2   | ResearchPapers | 0.97063 | 0.95158 | 0.95976 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   4    |   20    |   0.206735   |   29.27  \n",
            "   4    |   40    |   0.205634   |   27.88  \n",
            "   4    |   60    |   0.203233   |   27.87  \n",
            "   4    |   80    |   0.198655   |   27.85  \n",
            "   4    |   100   |   0.192067   |   27.82  \n",
            "   4    |   120   |   0.195513   |   27.89  \n",
            "   4    |   140   |   0.190585   |   27.85  \n",
            "   4    |   160   |   0.192024   |   27.93  \n",
            "   4    |   178   |   0.188452   |   24.58  \n",
            "Average Train Loss: 0.19713859851133889\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   4    |   2   | ResearchPapers | 0.97008 | 0.95386 | 0.95871 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   5    |   20    |   0.183908   |   29.18  \n",
            "   5    |   40    |   0.177914   |   27.75  \n",
            "   5    |   60    |   0.182578   |   27.88  \n",
            "   5    |   80    |   0.177915   |   27.92  \n",
            "   5    |   100   |   0.177618   |   27.86  \n",
            "   5    |   120   |   0.169718   |   27.83  \n",
            "   5    |   140   |   0.170276   |   27.83  \n",
            "   5    |   160   |   0.170313   |   27.85  \n",
            "   5    |   178   |   0.170825   |   24.54  \n",
            "Average Train Loss: 0.17577408470588024\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   5    |   2   | ResearchPapers | 0.97748 | 0.97081 | 0.96886 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   6    |   20    |   0.166253   |   29.28  \n",
            "   6    |   40    |   0.165016   |   27.89  \n",
            "   6    |   60    |   0.161738   |   27.82  \n",
            "   6    |   80    |   0.161800   |   27.79  \n",
            "   6    |   100   |   0.161083   |   27.69  \n",
            "   6    |   120   |   0.160545   |   27.76  \n",
            "   6    |   140   |   0.155823   |   27.81  \n",
            "   6    |   160   |   0.154479   |   27.91  \n",
            "   6    |   178   |   0.157447   |   24.52  \n",
            "Average Train Loss: 0.16053089432876202\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   6    |   2   | ResearchPapers | 0.98160 | 0.96832 | 0.97096 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   7    |   20    |   0.153562   |   29.17  \n",
            "   7    |   40    |   0.153649   |   27.75  \n",
            "   7    |   60    |   0.150900   |   27.80  \n",
            "   7    |   80    |   0.150787   |   27.87  \n",
            "   7    |   100   |   0.149549   |   27.90  \n",
            "   7    |   120   |   0.148700   |   27.88  \n",
            "   7    |   140   |   0.147834   |   27.74  \n",
            "   7    |   160   |   0.145167   |   27.78  \n",
            "   7    |   178   |   0.150489   |   24.55  \n",
            "Average Train Loss: 0.15008555094623033\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   7    |   2   | ResearchPapers | 0.97986 | 0.97193 | 0.97026 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   8    |   20    |   0.145871   |   29.19  \n",
            "   8    |   40    |   0.144551   |   27.84  \n",
            "   8    |   60    |   0.145660   |   27.83  \n",
            "   8    |   80    |   0.146190   |   27.83  \n",
            "   8    |   100   |   0.146904   |   27.79  \n",
            "   8    |   120   |   0.143087   |   27.82  \n",
            "   8    |   140   |   0.140685   |   27.84  \n",
            "   8    |   160   |   0.138172   |   27.91  \n",
            "   8    |   178   |   0.135350   |   24.51  \n",
            "Average Train Loss: 0.14304224360089063\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   8    |   2   | ResearchPapers | 0.98124 | 0.96885 | 0.97061 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   9    |   20    |   0.142852   |   29.17  \n",
            "   9    |   40    |   0.141226   |   27.92  \n",
            "   9    |   60    |   0.137187   |   27.89  \n",
            "   9    |   80    |   0.137531   |   27.78  \n",
            "   9    |   100   |   0.136908   |   27.74  \n",
            "   9    |   120   |   0.134218   |   27.77  \n",
            "   9    |   140   |   0.137650   |   27.93  \n",
            "   9    |   160   |   0.137873   |   27.85  \n",
            "   9    |   178   |   0.140067   |   24.51  \n",
            "Average Train Loss: 0.1383961856614944\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   9    |   2   | ResearchPapers | 0.98362 | 0.96776 | 0.97201 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "  10    |   20    |   0.135798   |   29.11  \n",
            "  10    |   40    |   0.138395   |   27.85  \n",
            "  10    |   60    |   0.137827   |   27.90  \n",
            "  10    |   80    |   0.132044   |   27.85  \n",
            "  10    |   100   |   0.134320   |   27.87  \n",
            "  10    |   120   |   0.137104   |   27.80  \n",
            "  10    |   140   |   0.134958   |   27.75  \n",
            "  10    |   160   |   0.137678   |   27.79  \n",
            "  10    |   178   |   0.134858   |   24.44  \n",
            "Average Train Loss: 0.1358978568092405\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "  10    |   2   | ResearchPapers | 0.98294 | 0.96929 | 0.97236 \n",
            "Cross-Validation-Batch: 3\n",
            "Start training...\n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   20    |   0.551599   |   29.55  \n",
            "   1    |   40    |   0.508364   |   28.11  \n",
            "   1    |   60    |   0.466251   |   28.00  \n",
            "   1    |   80    |   0.436804   |   28.07  \n",
            "   1    |   100   |   0.395401   |   28.05  \n",
            "   1    |   120   |   0.368845   |   27.94  \n",
            "   1    |   140   |   0.351920   |   28.06  \n",
            "   1    |   160   |   0.333166   |   28.03  \n",
            "   1    |   178   |   0.318853   |   24.68  \n",
            "Average Train Loss: 0.41641304080046754\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   1    |   3   | ResearchPapers | 0.97085 | 0.71492 | 0.89114 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   2    |   20    |   0.307130   |   29.39  \n",
            "   2    |   40    |   0.294665   |   28.03  \n",
            "   2    |   60    |   0.288269   |   27.99  \n",
            "   2    |   80    |   0.276965   |   28.01  \n",
            "   2    |   100   |   0.267835   |   28.03  \n",
            "   2    |   120   |   0.258697   |   28.11  \n",
            "   2    |   140   |   0.252493   |   27.93  \n",
            "   2    |   160   |   0.242785   |   27.89  \n",
            "   2    |   178   |   0.241661   |   24.59  \n",
            "Average Train Loss: 0.2705801111859316\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   2    |   3   | ResearchPapers | 0.97751 | 0.90593 | 0.95065 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   3    |   20    |   0.231767   |   29.38  \n",
            "   3    |   40    |   0.222525   |   27.93  \n",
            "   3    |   60    |   0.222170   |   27.93  \n",
            "   3    |   80    |   0.215059   |   28.01  \n",
            "   3    |   100   |   0.215082   |   28.05  \n",
            "   3    |   120   |   0.209055   |   27.99  \n",
            "   3    |   140   |   0.207758   |   27.94  \n",
            "   3    |   160   |   0.208394   |   27.98  \n",
            "   3    |   178   |   0.203166   |   24.60  \n",
            "Average Train Loss: 0.21522331129572245\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   3    |   3   | ResearchPapers | 0.97369 | 0.95164 | 0.95730 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   4    |   20    |   0.192587   |   29.28  \n",
            "   4    |   40    |   0.193410   |   27.99  \n",
            "   4    |   60    |   0.187780   |   27.95  \n",
            "   4    |   80    |   0.187017   |   27.95  \n",
            "   4    |   100   |   0.184602   |   27.94  \n",
            "   4    |   120   |   0.186473   |   27.90  \n",
            "   4    |   140   |   0.180130   |   27.92  \n",
            "   4    |   160   |   0.177111   |   27.92  \n",
            "   4    |   178   |   0.175673   |   24.54  \n",
            "Average Train Loss: 0.18512237521523203\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   4    |   3   | ResearchPapers | 0.98367 | 0.95579 | 0.96815 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   5    |   20    |   0.167234   |   29.31  \n",
            "   5    |   40    |   0.170861   |   28.02  \n",
            "   5    |   60    |   0.169324   |   27.94  \n",
            "   5    |   80    |   0.166429   |   27.97  \n",
            "   5    |   100   |   0.166053   |   27.89  \n",
            "   5    |   120   |   0.163247   |   27.92  \n",
            "   5    |   140   |   0.161824   |   27.91  \n",
            "   5    |   160   |   0.158324   |   27.84  \n",
            "   5    |   178   |   0.159778   |   24.58  \n",
            "Average Train Loss: 0.16485553879977605\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   5    |   3   | ResearchPapers | 0.98365 | 0.96958 | 0.97200 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   6    |   20    |   0.158874   |   29.35  \n",
            "   6    |   40    |   0.155572   |   27.99  \n",
            "   6    |   60    |   0.154189   |   27.90  \n",
            "   6    |   80    |   0.152685   |   27.93  \n",
            "   6    |   100   |   0.150965   |   27.85  \n",
            "   6    |   120   |   0.146427   |   27.84  \n",
            "   6    |   140   |   0.149030   |   27.84  \n",
            "   6    |   160   |   0.145102   |   27.97  \n",
            "   6    |   178   |   0.148474   |   24.65  \n",
            "Average Train Loss: 0.15133123649232214\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   6    |   3   | ResearchPapers | 0.98605 | 0.96885 | 0.97410 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   7    |   20    |   0.145851   |   29.30  \n",
            "   7    |   40    |   0.142245   |   27.85  \n",
            "   7    |   60    |   0.141777   |   27.82  \n",
            "   7    |   80    |   0.141191   |   27.84  \n",
            "   7    |   100   |   0.141796   |   27.85  \n",
            "   7    |   120   |   0.140864   |   27.90  \n",
            "   7    |   140   |   0.137733   |   27.93  \n",
            "   7    |   160   |   0.140189   |   27.94  \n",
            "   7    |   178   |   0.136603   |   24.55  \n",
            "Average Train Loss: 0.1409923501687343\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   7    |   3   | ResearchPapers | 0.98851 | 0.97169 | 0.97620 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   8    |   20    |   0.138218   |   29.22  \n",
            "   8    |   40    |   0.134035   |   27.88  \n",
            "   8    |   60    |   0.134709   |   27.86  \n",
            "   8    |   80    |   0.136036   |   27.87  \n",
            "   8    |   100   |   0.135010   |   27.93  \n",
            "   8    |   120   |   0.134839   |   27.95  \n",
            "   8    |   140   |   0.129727   |   27.84  \n",
            "   8    |   160   |   0.135136   |   27.84  \n",
            "   8    |   178   |   0.130796   |   24.51  \n",
            "Average Train Loss: 0.13433943035382798\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   8    |   3   | ResearchPapers | 0.98625 | 0.97374 | 0.97515 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   9    |   20    |   0.131025   |   29.33  \n",
            "   9    |   40    |   0.131647   |   27.90  \n",
            "   9    |   60    |   0.128102   |   27.91  \n",
            "   9    |   80    |   0.128595   |   27.91  \n",
            "   9    |   100   |   0.130666   |   27.95  \n",
            "   9    |   120   |   0.132159   |   27.91  \n",
            "   9    |   140   |   0.127995   |   27.86  \n",
            "   9    |   160   |   0.127244   |   27.83  \n",
            "   9    |   178   |   0.130172   |   24.55  \n",
            "Average Train Loss: 0.12973616355457784\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   9    |   3   | ResearchPapers | 0.98880 | 0.97055 | 0.97410 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "  10    |   20    |   0.129599   |   29.24  \n",
            "  10    |   40    |   0.126865   |   27.91  \n",
            "  10    |   60    |   0.125320   |   27.98  \n",
            "  10    |   80    |   0.127023   |   27.99  \n",
            "  10    |   100   |   0.127450   |   27.93  \n",
            "  10    |   120   |   0.127710   |   27.87  \n",
            "  10    |   140   |   0.129043   |   27.88  \n",
            "  10    |   160   |   0.127925   |   27.92  \n",
            "  10    |   178   |   0.125882   |   24.60  \n",
            "Average Train Loss: 0.1274532671997001\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "  10    |   3   | ResearchPapers | 0.98612 | 0.97202 | 0.97410 \n",
            "Cross-Validation-Batch: 4\n",
            "Start training...\n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   20    |   0.520968   |   29.40  \n",
            "   1    |   40    |   0.487716   |   28.00  \n",
            "   1    |   60    |   0.464278   |   27.90  \n",
            "   1    |   80    |   0.436068   |   28.00  \n",
            "   1    |   100   |   0.403115   |   28.06  \n",
            "   1    |   120   |   0.377656   |   28.03  \n",
            "   1    |   140   |   0.350815   |   28.06  \n",
            "   1    |   160   |   0.332148   |   28.03  \n",
            "   1    |   178   |   0.310720   |   24.69  \n",
            "Average Train Loss: 0.41100118413317804\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   1    |   4   | ResearchPapers | 0.96879 | 0.71463 | 0.86699 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   2    |   20    |   0.294225   |   29.40  \n",
            "   2    |   40    |   0.279858   |   28.05  \n",
            "   2    |   60    |   0.278056   |   28.02  \n",
            "   2    |   80    |   0.266301   |   28.04  \n",
            "   2    |   100   |   0.261528   |   28.00  \n",
            "   2    |   120   |   0.246818   |   28.00  \n",
            "   2    |   140   |   0.244627   |   28.00  \n",
            "   2    |   160   |   0.236908   |   27.99  \n",
            "   2    |   178   |   0.236653   |   24.64  \n",
            "Average Train Loss: 0.26100786305006657\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   2    |   4   | ResearchPapers | 0.96507 | 0.96067 | 0.95030 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   3    |   20    |   0.223824   |   29.29  \n",
            "   3    |   40    |   0.218960   |   28.02  \n",
            "   3    |   60    |   0.215502   |   28.02  \n",
            "   3    |   80    |   0.213537   |   28.04  \n",
            "   3    |   100   |   0.211739   |   28.01  \n",
            "   3    |   120   |   0.207655   |   27.99  \n",
            "   3    |   140   |   0.201357   |   27.98  \n",
            "   3    |   160   |   0.198546   |   28.00  \n",
            "   3    |   178   |   0.192750   |   24.66  \n",
            "Average Train Loss: 0.20958525758215835\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   3    |   4   | ResearchPapers | 0.97350 | 0.96627 | 0.95415 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   4    |   20    |   0.190422   |   29.34  \n",
            "   4    |   40    |   0.188835   |   28.04  \n",
            "   4    |   60    |   0.183426   |   28.02  \n",
            "   4    |   80    |   0.183879   |   28.05  \n",
            "   4    |   100   |   0.181503   |   28.04  \n",
            "   4    |   120   |   0.180362   |   28.00  \n",
            "   4    |   140   |   0.176569   |   27.96  \n",
            "   4    |   160   |   0.171129   |   27.97  \n",
            "   4    |   178   |   0.171711   |   24.64  \n",
            "Average Train Loss: 0.181026370831708\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   4    |   4   | ResearchPapers | 0.96974 | 0.97432 | 0.95590 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   5    |   20    |   0.166319   |   29.38  \n",
            "   5    |   40    |   0.167081   |   28.03  \n",
            "   5    |   60    |   0.164483   |   27.94  \n",
            "   5    |   80    |   0.164050   |   27.89  \n",
            "   5    |   100   |   0.163021   |   28.00  \n",
            "   5    |   120   |   0.163594   |   28.02  \n",
            "   5    |   140   |   0.156102   |   27.96  \n",
            "   5    |   160   |   0.157302   |   27.90  \n",
            "   5    |   178   |   0.155359   |   24.56  \n",
            "Average Train Loss: 0.1620213714891306\n",
            " Epoch  | Fold  |  Data  |  Precision   |    Recall    |   Accuracy   \n",
            "   5    |   4   | ResearchPapers | 0.97591 | 0.97834 | 0.96745 \n",
            " Epoch  |  Batch  |  Train Loss  |   Val Loss   |  Elapsed \n",
            "--------------------------------------------------\n",
            "   6    |   20    |   0.151532   |   29.34  \n",
            "   6    |   40    |   0.150629   |   28.07  \n",
            "   6    |   60    |   0.147806   |   28.00  \n",
            "   6    |   80    |   0.150899   |   27.91  \n",
            "   6    |   100   |   0.150951   |   27.94  \n",
            "   6    |   120   |   0.147990   |   28.02  \n",
            "   6    |   140   |   0.146643   |   28.06  \n",
            "   6    |   160   |   0.147657   |   28.01  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AUBf2eB2wkW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}