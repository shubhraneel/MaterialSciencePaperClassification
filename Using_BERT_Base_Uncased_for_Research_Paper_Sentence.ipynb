{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Using BERT-Base-Uncased for Research Paper Sentence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75603d2c330d41b99389e3738128f3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f4460ac4af14da89d603fc76b722a0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28e605699cb64524bcd7cb13548f476b",
              "IPY_MODEL_15990cc1c8e846a094c4e0ac1f4d74d1"
            ]
          }
        },
        "0f4460ac4af14da89d603fc76b722a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28e605699cb64524bcd7cb13548f476b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8764d089cd694925ba761ce6c5fac125",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a7a335c7ed843c189c8d234775b7d28"
          }
        },
        "15990cc1c8e846a094c4e0ac1f4d74d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cda1a5175c66499aace2b9f8a52c6228",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 740kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d295b5786eda4f1089c78a2bbff82f20"
          }
        },
        "8764d089cd694925ba761ce6c5fac125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a7a335c7ed843c189c8d234775b7d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cda1a5175c66499aace2b9f8a52c6228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d295b5786eda4f1089c78a2bbff82f20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6d16ec991964d48ad1d145055ea9837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_adcf97dedce14c10a4a5cfbbe5be17f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_007ecd3d18be4f47b1226188caedb918",
              "IPY_MODEL_c7a2145ea05b4bedb1d6689f18c93712"
            ]
          }
        },
        "adcf97dedce14c10a4a5cfbbe5be17f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "007ecd3d18be4f47b1226188caedb918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8c52152f7284e7fb54569c6e7cba908",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58943c60d2ff4c008bc70c4e9c010631"
          }
        },
        "c7a2145ea05b4bedb1d6689f18c93712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89558aca10fe4f3cbc0e0f8b697f6805",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:08&lt;00:00, 53.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c220bb9c42040d9ad50b92c63aedc8a"
          }
        },
        "c8c52152f7284e7fb54569c6e7cba908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58943c60d2ff4c008bc70c4e9c010631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89558aca10fe4f3cbc0e0f8b697f6805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c220bb9c42040d9ad50b92c63aedc8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65fe5f53ab194a7ea50b6897b4896472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7e5a676d852436bbbd54005638c4833",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1144c2754748422ca60b0345a9c36e32",
              "IPY_MODEL_a6980db8f7124606910ff085007e10dd"
            ]
          }
        },
        "a7e5a676d852436bbbd54005638c4833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1144c2754748422ca60b0345a9c36e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc87d1a68b76457b836e073adf0f9d9c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_085f3d43f0854a0ba2396e2ab9450f7d"
          }
        },
        "a6980db8f7124606910ff085007e10dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c1f84f71c284fcfb086e38e7a0b0c3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 57.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2969c002482a4934a856249ba4946e2b"
          }
        },
        "dc87d1a68b76457b836e073adf0f9d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "085f3d43f0854a0ba2396e2ab9450f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c1f84f71c284fcfb086e38e7a0b0c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2969c002482a4934a856249ba4946e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js1Lt7xzp53p"
      },
      "source": [
        "# Using BERT-Base-Uncased for D-FJ and FactJudge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZ0kz1wqEk2"
      },
      "source": [
        "### Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rmv3qUboPtC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2RtnGzFp4cb"
      },
      "source": [
        "### Checking if GPU available\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaF0biAJqSVc",
        "outputId": "4e7c3962-7111-4a20-e1f0-ab13c74151e4"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'{torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVT6GHPmqvhM"
      },
      "source": [
        "### Downloading and Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmY3EHwDsyht",
        "outputId": "282d9f80-b338-4fc7-bf52-24c8531a39c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "vXh3pS1u9H8L",
        "outputId": "945ade3d-693a-44fd-dbb6-c702395fd86e"
      },
      "source": [
        "sentences_df = pd.read_csv(\"/content/gdrive/My Drive/SentencesLabeling/labeled_sentences.csv\")\n",
        "sentences_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "      <th>Subcat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We have investigated the electronic structure ...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It was found that the Sn valence states (5s, 5...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It was demonstrated that the metallic states a...</td>\n",
              "      <td>1</td>\n",
              "      <td>MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We discuss the nature of the electronic states...</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ar X\\niv :1\\n10 7.</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>34 11\\nv1 [\\nco nd\\n-m at\\n.m tr\\nlsc\\ni] 1\\n8...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lukoyanovâˆ—,âˆ—âˆ—, E. Z.Kurmaevâˆ—, L.D.</td>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Finkelsteinâˆ—, A.Moewes+\\n+Department of Physic...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD|MATERIAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It was found that the Sn valence states (5s, 5...</td>\n",
              "      <td>1</td>\n",
              "      <td>METHOD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>It was demonstrated that the metallic states a...</td>\n",
              "      <td>1</td>\n",
              "      <td>MATERIAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label           Subcat\n",
              "0  We have investigated the electronic structure ...      1  METHOD|MATERIAL\n",
              "1  It was found that the Sn valence states (5s, 5...      1           METHOD\n",
              "2  It was demonstrated that the metallic states a...      1         MATERIAL\n",
              "3  We discuss the nature of the electronic states...      0                O\n",
              "4                                 ar X\\niv :1\\n10 7.      0                O\n",
              "5  34 11\\nv1 [\\nco nd\\n-m at\\n.m tr\\nlsc\\ni] 1\\n8...      1  METHOD|MATERIAL\n",
              "6                 Lukoyanovâˆ—,âˆ—âˆ—, E. Z.Kurmaevâˆ—, L.D.      0                O\n",
              "7  Finkelsteinâˆ—, A.Moewes+\\n+Department of Physic...      1  METHOD|MATERIAL\n",
              "8  It was found that the Sn valence states (5s, 5...      1           METHOD\n",
              "9  It was demonstrated that the metallic states a...      1         MATERIAL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40uCzfV9ujse"
      },
      "source": [
        "### Importing BERT Model Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rjRgryxunRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b81b16-d07a-4f27-8c5d-57e63fd076f8"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=74b00a2873d64d24ec8cbcec750ec94b1c202f763063797bdf847247d704c941\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Q_t9WKacvU"
      },
      "source": [
        "### Importing sklearn requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqBl6bh8ab5U"
      },
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkVYOihYuaOq"
      },
      "source": [
        "### BERT Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqMXPBluY9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "75603d2c330d41b99389e3738128f3d6",
            "0f4460ac4af14da89d603fc76b722a0f",
            "28e605699cb64524bcd7cb13548f476b",
            "15990cc1c8e846a094c4e0ac1f4d74d1",
            "8764d089cd694925ba761ce6c5fac125",
            "2a7a335c7ed843c189c8d234775b7d28",
            "cda1a5175c66499aace2b9f8a52c6228",
            "d295b5786eda4f1089c78a2bbff82f20"
          ]
        },
        "outputId": "d96259d2-3ba7-4afa-9219-4872bcefe853"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "def preprocess_bert(data):\n",
        "  encoded = tokenizer.encode_plus(\n",
        "      text = data,\n",
        "      add_special_tokens = True,\n",
        "      max_length = MAX_LEN,\n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "  return encoded.get('input_ids'), encoded.get('attention_mask')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75603d2c330d41b99389e3738128f3d6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqZ-gF4kBHU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "c9e5f8e3-3973-4b62-c087-cd025f18afd6"
      },
      "source": [
        "all_sentences = list(sentences_df[\"Sentence\"])\n",
        "encoded_sents = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]\n",
        "plt.hist([len(sent) for sent in encoded_sents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.308e+04, 3.611e+03, 3.530e+02, 1.220e+02, 3.700e+01, 9.000e+00,\n",
              "        2.000e+00, 4.000e+00, 1.000e+00, 2.000e+00]),\n",
              " array([  3. ,  60.6, 118.2, 175.8, 233.4, 291. , 348.6, 406.2, 463.8,\n",
              "        521.4, 579. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW5GiVatkeNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231e5c46-2761-47d0-90b8-039839d3272e"
      },
      "source": [
        "MAX_LEN = 128\n",
        "sentences_df['input_ids'], sentences_df['attention_mask'] = zip(*sentences_df['Sentence'].apply(preprocess_bert))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4JqCeoUbaNj"
      },
      "source": [
        "### Separating preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl8Gsf4dpzAC"
      },
      "source": [
        "# mqpa_group = mqpa_train_df.groupby('Document No.')\n",
        "# mqpa_inputids_document_wise = mqpa_group['input_ids'].apply(np.array)\n",
        "# mqpa_masks_document_wise = mqpa_group['attention_mask'].apply(np.array)\n",
        "# mqpa_ytrain_document_wise = mqpa_group['Label'].apply(np.array)\n",
        "\n",
        "# yahoo_group = yahoo_train_df.groupby('Document No.')\n",
        "# yahoo_inputids_document_wise = yahoo_group['input_ids'].apply(np.array)\n",
        "# yahoo_masks_document_wise = yahoo_group['attention_mask'].apply(np.array)\n",
        "# yahoo_ytrain_document_wise = yahoo_group['Label'].apply(np.array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgwMxYsfbfzv"
      },
      "source": [
        "### Fixed Random State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QbfXSlw7mSE"
      },
      "source": [
        "RANDOM_STATE = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-5SGYfbbjwI"
      },
      "source": [
        "### Creating the final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKkHgO4G_I0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d9eb0d-7aea-45d4-bc9c-bba18506addb"
      },
      "source": [
        "%%time\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, freeze_bert=False, RNNLayer='RNN', activation='Sigmoid', hidden_nodes=50):\n",
        "    super(BertClassifier, self).__init__()\n",
        "\n",
        "    D_in, H, D_out = 768, hidden_nodes, 2\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    if RNNLayer == 'RNN':\n",
        "      self.rnn = nn.RNN(D_in, H, batch_first = True)\n",
        "    elif RNNLayer == 'LSTM':\n",
        "      self.rnn = nn.LSTM(D_in, H, batch_first = True)\n",
        "    self.linear = nn.Linear(H, D_out)\n",
        "    if activation == 'Sigmoid':\n",
        "      self.activation = nn.Sigmoid()\n",
        "    elif activation == 'ReLU':\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == 'Tanh':\n",
        "      self.activation = nn.Tanh()\n",
        "\n",
        "    modules = [self.bert.embeddings, *self.bert.encoder.layer[:5]]\n",
        "    if freeze_bert:\n",
        "      for module in modules:\n",
        "        for param in module.parameters():\n",
        "          param.requires_grad = False\n",
        "  \n",
        "  def forward(self, input_ids, attention_masks):\n",
        "\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_masks)\n",
        "    last_hidden_state = outputs[0]\n",
        "    rnn_out, _ = self.rnn(last_hidden_state)\n",
        "    # activated = self.activation(rnn_out[:, -1, :])\n",
        "    logits = self.linear(rnn_out[:, -1, :])\n",
        "    # logits = self.linear(last_hidden_state)\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 26 Âµs, sys: 0 ns, total: 26 Âµs\n",
            "Wall time: 28.4 Âµs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur67S-S2buGf"
      },
      "source": [
        "### Train Test k-fold splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU57AIi5GRmS"
      },
      "source": [
        "def KFoldIds(X_train, labels, k=5):\n",
        "  kf = StratifiedKFold(n_splits=k, random_state=RANDOM_STATE, shuffle=True)\n",
        "\n",
        "  train_test_splits = []\n",
        "  for train_index, test_index in kf.split(X_train, labels): # splitting in ratio of documents\n",
        "    # arrays containing arrays of sentences in each document\n",
        "    train_test_splits.append((train_index, test_index))\n",
        "  \n",
        "  return train_test_splits\n",
        "\n",
        "def create_train_test_datasets(input_ids, masks, labels, k=5):\n",
        "  input_ids_final = []\n",
        "  masks_final = []\n",
        "  labels_final = []\n",
        "  input_ids_final_test = []\n",
        "  masks_final_test = []\n",
        "  labels_final_test = []\n",
        "\n",
        "  train_test_splits = KFoldIds(input_ids, labels, k)\n",
        "  for train_index, test_index in train_test_splits:\n",
        "    input_ids_tr, input_ids_test = input_ids[train_index], input_ids[test_index]\n",
        "    masks_tr, masks_test = masks[train_index], masks[test_index]\n",
        "    labels_tr, labels_test = labels[train_index], labels[test_index]\n",
        "    # concatenate all arrays together for training purpose\n",
        "    input_ids_tensor = torch.tensor(list(input_ids_tr))\n",
        "    masks_tensor = torch.tensor(list(masks_tr))\n",
        "    labels_tensor = torch.tensor(list(labels_tr))\n",
        "    input_ids_tensor_test = torch.tensor(list(input_ids_test))\n",
        "    masks_tensor_test = torch.tensor(list(masks_test))\n",
        "    labels_tensor_test = torch.tensor(list(labels_test))\n",
        "    # store all the divided parts for reusing again and again\n",
        "    input_ids_final.append(input_ids_tensor)\n",
        "    masks_final.append(masks_tensor)\n",
        "    labels_final.append(labels_tensor)\n",
        "    input_ids_final_test.append(input_ids_tensor_test)\n",
        "    masks_final_test.append(masks_tensor_test)\n",
        "    labels_final_test.append(labels_tensor_test)\n",
        "  \n",
        "  return input_ids_final, masks_final, labels_final, input_ids_final_test, masks_final_test, labels_final_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRVjGpdEb7_9"
      },
      "source": [
        "## Model initializer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXuJg7QTwr6K"
      },
      "source": [
        "def initialize_model(RNNLayer='RNN', activation='Sigmoid'):\n",
        "  # Instantiate Bert Classifier\n",
        "  bert_classifier = BertClassifier(freeze_bert=False, RNNLayer=RNNLayer, activation=activation)\n",
        "\n",
        "  # Tell PyTorch to run the model on GPU\n",
        "  bert_classifier.to(device)\n",
        "\n",
        "  # Create the optimizer\n",
        "  optimizer = AdamW(bert_classifier.parameters(),\n",
        "                    lr=5e-5,    \n",
        "                    eps=1e-8   \n",
        "                    )\n",
        "  \n",
        "  return bert_classifier, optimizer\n",
        "\n",
        "def get_scheduler(train_dataloader, optimizer, epochs=4):\n",
        "  # Total number of training steps\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Set up the learning rate scheduler\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps=0, \n",
        "                                              num_training_steps=total_steps)\n",
        "  return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUZC4Go6cF0X"
      },
      "source": [
        "### Training method for a combination of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISs6NEeK19dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693ccf6e-93a5-4d08-e388-9c4bc4abf9df"
      },
      "source": [
        "%%time\n",
        "\n",
        "def train(model, train_dataloader, optimizer, scheduler, epochs=4, test_ids = None, test_masks = None, test_labels = None):\n",
        "  print(\"Start training...\")\n",
        "  # loss_fn = nn.BCELoss()\n",
        "  loss_fn=nn.CrossEntropyLoss()\n",
        "  for epoch_i in range(epochs):\n",
        "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Elapsed':^9}\")\n",
        "    print('-'*50)\n",
        "\n",
        "    t0_epoch, t0_batch = time.time(), time.time()\n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts +=1\n",
        "      # Load batch to GPU\n",
        "      b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "      # Zero out any previously calculated gradients\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Perform a forward pass. This will return logits.\n",
        "      logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "      # Compute loss and accumulate the loss values\n",
        "      # loss = loss_fn(logits, b_labels.to(torch.float32).unsqueeze(1))\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters and the learning rate\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "        # Calculate time elapsed for 20 batches\n",
        "        time_elapsed = time.time() - t0_batch\n",
        "\n",
        "        # Print training results\n",
        "        print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "        # Reset batch tracking variables\n",
        "        batch_loss, batch_counts = 0, 0\n",
        "        t0_batch = time.time()\n",
        "      \n",
        "    all_probs = bert_predict(model, test_ids, test_masks, test_labels)\n",
        "    all_probs = F.softmax(torch.cat(all_probs), dim=1).cpu().numpy()\n",
        "    precision, recall, thresholds = precision_recall_curve(np.array(test_labels), all_probs[:, 1])\n",
        "    fscore = (2 * precision * recall) / (precision + recall)\n",
        "    ix = np.nanargmax(fscore)\n",
        "    accuracy = accuracy_score(np.array(test_labels), np.where(all_probs[:,1]>=thresholds[ix], 1, 0))\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Average Train Loss: {avg_train_loss}')\n",
        "\n",
        "    # Printing results for these hyperparameters\n",
        "    print(f\"{'Epochs':^10} | {'Precision':^15} | {'Recall':^15} | {'Accuracy':^15} | {'Threshold':^15}\")\n",
        "\n",
        "    print(f\"{epoch_i:^10} | {precision[ix]:^15.2f} | {recall[ix]:^15.2f} | {accuracy:^15.2f} | {thresholds[ix]:^15.2f}\")\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
            "Wall time: 5.48 Âµs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnIqYUgKOgv"
      },
      "source": [
        "def set_seed(seed_value=42):\n",
        "  \"\"\"Set seed for reproducibility.\n",
        "  \"\"\"\n",
        "  random.seed(seed_value)\n",
        "  np.random.seed(seed_value)\n",
        "  torch.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5w0SIXZcOkn"
      },
      "source": [
        "### Predict with trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxRj83YMMHk"
      },
      "source": [
        "def bert_predict(model, input_ids, masks, labels):\n",
        "  test_data = TensorDataset(input_ids, masks, labels)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "  all_probs=[]\n",
        "  for batch in test_dataloader:\n",
        "    # Load batch to GPU\n",
        "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "    # Compute logits\n",
        "    with torch.no_grad():\n",
        "      probs = model(b_input_ids, b_attn_mask)\n",
        "    all_probs.append(probs)\n",
        "\n",
        "  return all_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVL6nQuOYwFF"
      },
      "source": [
        "def train_loop():\n",
        "  \n",
        "  k = 5\n",
        "\n",
        "  input_ids_final, masks_final, labels_final, \\\n",
        "    input_ids_final_test, masks_final_test, labels_final_test = \\\n",
        "    create_train_test_datasets(np.array(sentences_df[\"input_ids\"]), np.array(sentences_df[\"attention_mask\"]), np.array(sentences_df[\"Label\"]))\n",
        "\n",
        "  # Best parameters RNN, 32 batch size, 9 epochs\n",
        "  RNNLayers = ['RNN']\n",
        "  batch_sizes = [32]\n",
        "  epochs_s = [9]\n",
        "  activations = ['Sigmoid']\n",
        "\n",
        "  for RNNLayer, batch_size, epochs, activation in list(itertools.product(RNNLayers, batch_sizes, epochs_s, activations)):\n",
        "\n",
        "    time0 = time.time()\n",
        "\n",
        "    print(f\"{'Model':^7} | {'Activation':^15} {'Batch Size':^15} | {'Epochs':^10}\")\n",
        "    print(f\"{RNNLayer:^7} | {activation:^15} {batch_size:^15} | {epochs:^10}\")\n",
        "\n",
        "    for i in range(k):\n",
        "\n",
        "      train_data = TensorDataset(input_ids_final[i], masks_final[i], labels_final[i])\n",
        "      train_sampler = RandomSampler(train_data)\n",
        "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "      bert_classifier, optimizer = initialize_model(RNNLayer=RNNLayer, activation=activation)\n",
        "      scheduler = get_scheduler(train_dataloader, optimizer, epochs=epochs)\n",
        "      model = train(bert_classifier, train_dataloader, optimizer, scheduler, epochs, \n",
        "                    input_ids_final_test[i], masks_final_test[i], labels_final_test[i])\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    train_time = datetime.timedelta(seconds=time_end-time0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDcLFRCvdKFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d6d16ec991964d48ad1d145055ea9837",
            "adcf97dedce14c10a4a5cfbbe5be17f0",
            "007ecd3d18be4f47b1226188caedb918",
            "c7a2145ea05b4bedb1d6689f18c93712",
            "c8c52152f7284e7fb54569c6e7cba908",
            "58943c60d2ff4c008bc70c4e9c010631",
            "89558aca10fe4f3cbc0e0f8b697f6805",
            "5c220bb9c42040d9ad50b92c63aedc8a",
            "65fe5f53ab194a7ea50b6897b4896472",
            "a7e5a676d852436bbbd54005638c4833",
            "1144c2754748422ca60b0345a9c36e32",
            "a6980db8f7124606910ff085007e10dd",
            "dc87d1a68b76457b836e073adf0f9d9c",
            "085f3d43f0854a0ba2396e2ab9450f7d",
            "2c1f84f71c284fcfb086e38e7a0b0c3f",
            "2969c002482a4934a856249ba4946e2b"
          ]
        },
        "outputId": "d20dfef0-5569-4cc5-eca4-38f842d2e039"
      },
      "source": [
        "set_seed(42)\n",
        "train_loop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Model  |   Activation      Batch Size    |   Epochs  \n",
            " LSTM   |     Sigmoid           16        |     9     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6d16ec991964d48ad1d145055ea9837",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65fe5f53ab194a7ea50b6897b4896472",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training...\n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   20    |   0.531981   |   7.72   \n",
            "   1    |   40    |   0.465218   |   7.23   \n",
            "   1    |   60    |   0.399220   |   7.30   \n",
            "   1    |   80    |   0.419708   |   7.32   \n",
            "   1    |   100   |   0.392539   |   7.40   \n",
            "   1    |   120   |   0.389412   |   7.49   \n",
            "   1    |   140   |   0.390482   |   7.61   \n",
            "   1    |   160   |   0.346109   |   7.72   \n",
            "   1    |   180   |   0.355398   |   7.73   \n",
            "   1    |   200   |   0.315146   |   7.83   \n",
            "   1    |   220   |   0.333137   |   7.95   \n",
            "   1    |   240   |   0.316178   |   8.10   \n",
            "   1    |   260   |   0.282041   |   8.15   \n",
            "   1    |   280   |   0.349180   |   8.11   \n",
            "   1    |   300   |   0.311165   |   7.98   \n",
            "   1    |   320   |   0.309367   |   7.91   \n",
            "   1    |   340   |   0.301792   |   7.83   \n",
            "   1    |   360   |   0.290493   |   7.85   \n",
            "   1    |   380   |   0.342188   |   7.88   \n",
            "   1    |   400   |   0.298216   |   7.92   \n",
            "   1    |   420   |   0.351823   |   7.96   \n",
            "   1    |   440   |   0.277587   |   7.99   \n",
            "   1    |   460   |   0.275539   |   8.00   \n",
            "   1    |   480   |   0.211470   |   7.98   \n",
            "   1    |   500   |   0.182749   |   7.99   \n",
            "   1    |   520   |   0.220150   |   7.96   \n",
            "   1    |   540   |   0.153550   |   7.94   \n",
            "   1    |   560   |   0.270833   |   7.87   \n",
            "   1    |   580   |   0.249718   |   7.90   \n",
            "   1    |   600   |   0.321829   |   7.91   \n",
            "   1    |   620   |   0.214617   |   7.92   \n",
            "   1    |   640   |   0.232066   |   7.94   \n",
            "   1    |   660   |   0.154243   |   7.93   \n",
            "   1    |   680   |   0.238526   |   7.96   \n",
            "   1    |   700   |   0.202646   |   7.98   \n",
            "   1    |   720   |   0.218595   |   7.98   \n",
            "   1    |   740   |   0.162794   |   7.99   \n",
            "   1    |   760   |   0.156126   |   7.95   \n",
            "   1    |   780   |   0.207359   |   7.95   \n",
            "   1    |   800   |   0.225373   |   7.93   \n",
            "   1    |   820   |   0.163864   |   7.91   \n",
            "   1    |   840   |   0.186525   |   7.94   \n",
            "   1    |   860   |   0.256580   |   7.93   \n",
            "   1    |   880   |   0.157774   |   7.92   \n",
            "   1    |   900   |   0.183534   |   7.92   \n",
            "   1    |   920   |   0.180611   |   7.91   \n",
            "   1    |   940   |   0.105544   |   7.92   \n",
            "   1    |   960   |   0.166634   |   7.87   \n",
            "   1    |   980   |   0.124520   |   7.89   \n",
            "   1    |  1000   |   0.164501   |   7.93   \n",
            "   1    |  1020   |   0.160583   |   7.91   \n",
            "   1    |  1040   |   0.125822   |   7.93   \n",
            "   1    |  1060   |   0.151852   |   7.90   \n",
            "   1    |  1080   |   0.194915   |   7.91   \n",
            "   1    |  1100   |   0.153819   |   7.90   \n",
            "   1    |  1120   |   0.137573   |   7.91   \n",
            "   1    |  1140   |   0.111858   |   7.92   \n",
            "   1    |  1160   |   0.237921   |   7.93   \n",
            "   1    |  1180   |   0.173064   |   7.92   \n",
            "   1    |  1200   |   0.181776   |   7.93   \n",
            "   1    |  1220   |   0.206772   |   7.92   \n",
            "   1    |  1240   |   0.135648   |   7.90   \n",
            "   1    |  1260   |   0.202198   |   7.91   \n",
            "   1    |  1280   |   0.141963   |   7.90   \n",
            "   1    |  1300   |   0.195803   |   7.92   \n",
            "   1    |  1320   |   0.120918   |   7.93   \n",
            "   1    |  1340   |   0.113777   |   7.91   \n",
            "   1    |  1360   |   0.141066   |   7.92   \n",
            "   1    |  1380   |   0.114404   |   7.90   \n",
            "   1    |  1400   |   0.094941   |   7.92   \n",
            "   1    |  1420   |   0.151012   |   7.88   \n",
            "   1    |  1440   |   0.161604   |   7.91   \n",
            "   1    |  1460   |   0.204651   |   7.89   \n",
            "   1    |  1480   |   0.170045   |   7.92   \n",
            "   1    |  1500   |   0.147480   |   7.93   \n",
            "   1    |  1520   |   0.146174   |   7.89   \n",
            "   1    |  1540   |   0.140314   |   7.91   \n",
            "   1    |  1560   |   0.088246   |   7.89   \n",
            "   1    |  1580   |   0.190530   |   7.90   \n",
            "   1    |  1600   |   0.122358   |   7.90   \n",
            "   1    |  1620   |   0.176690   |   7.94   \n",
            "   1    |  1640   |   0.122024   |   7.92   \n",
            "   1    |  1660   |   0.144349   |   7.93   \n",
            "   1    |  1680   |   0.075417   |   7.90   \n",
            "   1    |  1700   |   0.091834   |   7.93   \n",
            "   1    |  1720   |   0.095750   |   7.93   \n",
            "   1    |  1740   |   0.068519   |   7.92   \n",
            "   1    |  1760   |   0.115293   |   7.91   \n",
            "   1    |  1780   |   0.168568   |   7.91   \n",
            "   1    |  1800   |   0.119292   |   7.91   \n",
            "   1    |  1820   |   0.089917   |   7.91   \n",
            "   1    |  1840   |   0.128094   |   7.93   \n",
            "   1    |  1860   |   0.108406   |   7.93   \n",
            "   1    |  1880   |   0.138439   |   7.93   \n",
            "   1    |  1900   |   0.163220   |   7.93   \n",
            "   1    |  1920   |   0.117694   |   7.90   \n",
            "   1    |  1940   |   0.124266   |   7.92   \n",
            "   1    |  1960   |   0.120834   |   7.92   \n",
            "   1    |  1980   |   0.164081   |   7.89   \n",
            "   1    |  2000   |   0.124900   |   7.89   \n",
            "   1    |  2020   |   0.257319   |   7.93   \n",
            "   1    |  2040   |   0.195120   |   7.89   \n",
            "   1    |  2060   |   0.169921   |   7.88   \n",
            "   1    |  2080   |   0.215847   |   7.90   \n",
            "   1    |  2100   |   0.141001   |   7.91   \n",
            "   1    |  2120   |   0.102723   |   7.92   \n",
            "   1    |  2140   |   0.148096   |   7.89   \n",
            "   1    |  2160   |   0.109743   |   7.90   \n",
            "   1    |  2180   |   0.171238   |   7.89   \n",
            "   1    |  2200   |   0.268657   |   7.90   \n",
            "   1    |  2220   |   0.195131   |   7.88   \n",
            "   1    |  2240   |   0.176925   |   7.88   \n",
            "   1    |  2260   |   0.120507   |   7.86   \n",
            "   1    |  2280   |   0.031855   |   7.84   \n",
            "   1    |  2300   |   0.140675   |   7.87   \n",
            "   1    |  2320   |   0.103745   |   7.88   \n",
            "   1    |  2340   |   0.183534   |   7.88   \n",
            "   1    |  2360   |   0.143926   |   7.90   \n",
            "Average Train Loss: 0.19767797690702224\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    0      |      0.96       |      0.91       |      0.96       |      0.86      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   2    |   20    |   0.138243   |   8.31   \n",
            "   2    |   40    |   0.097150   |   7.92   \n",
            "   2    |   60    |   0.082477   |   7.90   \n",
            "   2    |   80    |   0.157783   |   7.90   \n",
            "   2    |   100   |   0.107253   |   7.91   \n",
            "   2    |   120   |   0.109836   |   7.90   \n",
            "   2    |   140   |   0.111886   |   7.91   \n",
            "   2    |   160   |   0.069609   |   7.91   \n",
            "   2    |   180   |   0.083404   |   7.89   \n",
            "   2    |   200   |   0.078222   |   7.89   \n",
            "   2    |   220   |   0.165418   |   7.89   \n",
            "   2    |   240   |   0.086863   |   7.89   \n",
            "   2    |   260   |   0.120745   |   7.88   \n",
            "   2    |   280   |   0.065871   |   7.91   \n",
            "   2    |   300   |   0.147728   |   7.88   \n",
            "   2    |   320   |   0.063982   |   7.88   \n",
            "   2    |   340   |   0.119681   |   7.88   \n",
            "   2    |   360   |   0.129392   |   7.89   \n",
            "   2    |   380   |   0.098231   |   7.91   \n",
            "   2    |   400   |   0.122056   |   7.87   \n",
            "   2    |   420   |   0.108121   |   7.89   \n",
            "   2    |   440   |   0.141323   |   7.93   \n",
            "   2    |   460   |   0.073269   |   7.85   \n",
            "   2    |   480   |   0.091839   |   7.89   \n",
            "   2    |   500   |   0.083023   |   7.90   \n",
            "   2    |   520   |   0.081151   |   7.89   \n",
            "   2    |   540   |   0.167494   |   7.90   \n",
            "   2    |   560   |   0.098611   |   7.87   \n",
            "   2    |   580   |   0.094908   |   7.88   \n",
            "   2    |   600   |   0.088556   |   7.87   \n",
            "   2    |   620   |   0.047400   |   7.89   \n",
            "   2    |   640   |   0.100464   |   7.91   \n",
            "   2    |   660   |   0.144587   |   7.89   \n",
            "   2    |   680   |   0.087895   |   7.90   \n",
            "   2    |   700   |   0.149938   |   7.90   \n",
            "   2    |   720   |   0.133975   |   7.88   \n",
            "   2    |   740   |   0.110598   |   7.89   \n",
            "   2    |   760   |   0.106558   |   7.88   \n",
            "   2    |   780   |   0.130609   |   7.89   \n",
            "   2    |   800   |   0.142733   |   7.89   \n",
            "   2    |   820   |   0.091655   |   7.90   \n",
            "   2    |   840   |   0.114812   |   7.89   \n",
            "   2    |   860   |   0.122477   |   7.86   \n",
            "   2    |   880   |   0.104792   |   7.87   \n",
            "   2    |   900   |   0.134166   |   7.89   \n",
            "   2    |   920   |   0.094875   |   7.93   \n",
            "   2    |   940   |   0.095725   |   7.90   \n",
            "   2    |   960   |   0.068099   |   7.88   \n",
            "   2    |   980   |   0.122719   |   7.86   \n",
            "   2    |  1000   |   0.092465   |   7.86   \n",
            "   2    |  1020   |   0.147130   |   7.86   \n",
            "   2    |  1040   |   0.104871   |   7.87   \n",
            "   2    |  1060   |   0.156807   |   7.88   \n",
            "   2    |  1080   |   0.115824   |   7.89   \n",
            "   2    |  1100   |   0.072135   |   7.88   \n",
            "   2    |  1120   |   0.127018   |   7.86   \n",
            "   2    |  1140   |   0.091738   |   7.89   \n",
            "   2    |  1160   |   0.105150   |   7.90   \n",
            "   2    |  1180   |   0.077578   |   7.87   \n",
            "   2    |  1200   |   0.090693   |   7.87   \n",
            "   2    |  1220   |   0.080789   |   7.84   \n",
            "   2    |  1240   |   0.099104   |   7.88   \n",
            "   2    |  1260   |   0.065623   |   7.86   \n",
            "   2    |  1280   |   0.155529   |   7.88   \n",
            "   2    |  1300   |   0.067738   |   7.87   \n",
            "   2    |  1320   |   0.055188   |   7.84   \n",
            "   2    |  1340   |   0.102847   |   7.87   \n",
            "   2    |  1360   |   0.068509   |   7.86   \n",
            "   2    |  1380   |   0.047042   |   7.85   \n",
            "   2    |  1400   |   0.160982   |   7.88   \n",
            "   2    |  1420   |   0.123345   |   7.87   \n",
            "   2    |  1440   |   0.101156   |   7.89   \n",
            "   2    |  1460   |   0.088036   |   7.87   \n",
            "   2    |  1480   |   0.077004   |   7.87   \n",
            "   2    |  1500   |   0.142326   |   7.85   \n",
            "   2    |  1520   |   0.149012   |   7.88   \n",
            "   2    |  1540   |   0.098402   |   7.88   \n",
            "   2    |  1560   |   0.090602   |   7.89   \n",
            "   2    |  1580   |   0.138975   |   7.88   \n",
            "   2    |  1600   |   0.078793   |   7.88   \n",
            "   2    |  1620   |   0.089471   |   7.89   \n",
            "   2    |  1640   |   0.102621   |   7.86   \n",
            "   2    |  1660   |   0.078962   |   7.86   \n",
            "   2    |  1680   |   0.075335   |   7.86   \n",
            "   2    |  1700   |   0.100166   |   7.90   \n",
            "   2    |  1720   |   0.110955   |   7.89   \n",
            "   2    |  1740   |   0.133996   |   7.88   \n",
            "   2    |  1760   |   0.113331   |   7.88   \n",
            "   2    |  1780   |   0.124498   |   7.90   \n",
            "   2    |  1800   |   0.138918   |   7.91   \n",
            "   2    |  1820   |   0.113063   |   7.91   \n",
            "   2    |  1840   |   0.128201   |   7.89   \n",
            "   2    |  1860   |   0.102263   |   7.87   \n",
            "   2    |  1880   |   0.085908   |   7.90   \n",
            "   2    |  1900   |   0.108288   |   7.88   \n",
            "   2    |  1920   |   0.102167   |   7.90   \n",
            "   2    |  1940   |   0.176333   |   7.86   \n",
            "   2    |  1960   |   0.138759   |   7.87   \n",
            "   2    |  1980   |   0.192308   |   7.86   \n",
            "   2    |  2000   |   0.098695   |   7.86   \n",
            "   2    |  2020   |   0.118818   |   7.91   \n",
            "   2    |  2040   |   0.094912   |   7.88   \n",
            "   2    |  2060   |   0.158216   |   7.89   \n",
            "   2    |  2080   |   0.063856   |   7.89   \n",
            "   2    |  2100   |   0.070353   |   7.86   \n",
            "   2    |  2120   |   0.116739   |   7.88   \n",
            "   2    |  2140   |   0.129786   |   7.86   \n",
            "   2    |  2160   |   0.112333   |   7.87   \n",
            "   2    |  2180   |   0.129224   |   7.86   \n",
            "   2    |  2200   |   0.063947   |   7.88   \n",
            "   2    |  2220   |   0.068801   |   7.87   \n",
            "   2    |  2240   |   0.081504   |   7.90   \n",
            "   2    |  2260   |   0.131333   |   7.89   \n",
            "   2    |  2280   |   0.091892   |   7.90   \n",
            "   2    |  2300   |   0.107742   |   7.88   \n",
            "   2    |  2320   |   0.113472   |   7.88   \n",
            "   2    |  2340   |   0.056793   |   7.89   \n",
            "   2    |  2360   |   0.127538   |   7.88   \n",
            "Average Train Loss: 0.10689554712350517\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    1      |      0.97       |      0.93       |      0.97       |      0.97      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   3    |   20    |   0.063954   |   8.30   \n",
            "   3    |   40    |   0.070484   |   7.90   \n",
            "   3    |   60    |   0.117979   |   7.90   \n",
            "   3    |   80    |   0.118179   |   7.90   \n",
            "   3    |   100   |   0.051129   |   7.91   \n",
            "   3    |   120   |   0.100229   |   7.90   \n",
            "   3    |   140   |   0.055101   |   7.86   \n",
            "   3    |   160   |   0.063653   |   7.88   \n",
            "   3    |   180   |   0.088875   |   7.90   \n",
            "   3    |   200   |   0.071202   |   7.88   \n",
            "   3    |   220   |   0.073339   |   7.88   \n",
            "   3    |   240   |   0.215232   |   7.90   \n",
            "   3    |   260   |   0.076640   |   7.89   \n",
            "   3    |   280   |   0.084103   |   7.90   \n",
            "   3    |   300   |   0.054550   |   7.83   \n",
            "   3    |   320   |   0.104726   |   7.85   \n",
            "   3    |   340   |   0.035290   |   7.85   \n",
            "   3    |   360   |   0.124234   |   7.88   \n",
            "   3    |   380   |   0.057401   |   7.88   \n",
            "   3    |   400   |   0.110160   |   7.88   \n",
            "   3    |   420   |   0.018754   |   7.87   \n",
            "   3    |   440   |   0.045172   |   7.86   \n",
            "   3    |   460   |   0.103964   |   7.86   \n",
            "   3    |   480   |   0.127978   |   7.89   \n",
            "   3    |   500   |   0.068737   |   7.86   \n",
            "   3    |   520   |   0.050966   |   7.84   \n",
            "   3    |   540   |   0.061544   |   7.85   \n",
            "   3    |   560   |   0.054070   |   7.85   \n",
            "   3    |   580   |   0.082155   |   7.87   \n",
            "   3    |   600   |   0.064556   |   7.87   \n",
            "   3    |   620   |   0.034552   |   7.87   \n",
            "   3    |   640   |   0.067828   |   7.88   \n",
            "   3    |   660   |   0.064350   |   7.87   \n",
            "   3    |   680   |   0.065094   |   7.87   \n",
            "   3    |   700   |   0.082777   |   7.88   \n",
            "   3    |   720   |   0.138014   |   7.88   \n",
            "   3    |   740   |   0.076818   |   7.88   \n",
            "   3    |   760   |   0.082962   |   7.89   \n",
            "   3    |   780   |   0.032542   |   7.88   \n",
            "   3    |   800   |   0.068285   |   7.88   \n",
            "   3    |   820   |   0.080053   |   7.89   \n",
            "   3    |   840   |   0.042440   |   7.90   \n",
            "   3    |   860   |   0.081116   |   7.89   \n",
            "   3    |   880   |   0.094811   |   7.89   \n",
            "   3    |   900   |   0.027034   |   7.84   \n",
            "   3    |   920   |   0.087075   |   7.88   \n",
            "   3    |   940   |   0.095073   |   7.86   \n",
            "   3    |   960   |   0.066409   |   7.87   \n",
            "   3    |   980   |   0.107178   |   7.87   \n",
            "   3    |  1000   |   0.076833   |   7.85   \n",
            "   3    |  1020   |   0.161379   |   7.85   \n",
            "   3    |  1040   |   0.153877   |   7.86   \n",
            "   3    |  1060   |   0.091546   |   7.85   \n",
            "   3    |  1080   |   0.069933   |   7.85   \n",
            "   3    |  1100   |   0.098057   |   7.85   \n",
            "   3    |  1120   |   0.076672   |   7.87   \n",
            "   3    |  1140   |   0.155820   |   7.89   \n",
            "   3    |  1160   |   0.129600   |   7.86   \n",
            "   3    |  1180   |   0.077600   |   7.85   \n",
            "   3    |  1200   |   0.055829   |   7.87   \n",
            "   3    |  1220   |   0.187012   |   7.91   \n",
            "   3    |  1240   |   0.063336   |   7.87   \n",
            "   3    |  1260   |   0.108521   |   7.89   \n",
            "   3    |  1280   |   0.144566   |   7.84   \n",
            "   3    |  1300   |   0.093987   |   7.86   \n",
            "   3    |  1320   |   0.075709   |   7.86   \n",
            "   3    |  1340   |   0.071438   |   7.80   \n",
            "   3    |  1360   |   0.122949   |   7.86   \n",
            "   3    |  1380   |   0.043353   |   7.88   \n",
            "   3    |  1400   |   0.072830   |   7.87   \n",
            "   3    |  1420   |   0.071690   |   7.88   \n",
            "   3    |  1440   |   0.133427   |   7.88   \n",
            "   3    |  1460   |   0.081647   |   7.89   \n",
            "   3    |  1480   |   0.093073   |   7.90   \n",
            "   3    |  1500   |   0.055684   |   7.86   \n",
            "   3    |  1520   |   0.055607   |   7.86   \n",
            "   3    |  1540   |   0.107274   |   7.85   \n",
            "   3    |  1560   |   0.060344   |   7.89   \n",
            "   3    |  1580   |   0.114340   |   7.87   \n",
            "   3    |  1600   |   0.196226   |   7.85   \n",
            "   3    |  1620   |   0.087606   |   7.85   \n",
            "   3    |  1640   |   0.090298   |   7.86   \n",
            "   3    |  1660   |   0.114726   |   7.84   \n",
            "   3    |  1680   |   0.159459   |   7.86   \n",
            "   3    |  1700   |   0.096963   |   7.87   \n",
            "   3    |  1720   |   0.108134   |   7.86   \n",
            "   3    |  1740   |   0.101648   |   7.86   \n",
            "   3    |  1760   |   0.203277   |   7.85   \n",
            "   3    |  1780   |   0.143134   |   7.85   \n",
            "   3    |  1800   |   0.073250   |   7.87   \n",
            "   3    |  1820   |   0.068166   |   7.87   \n",
            "   3    |  1840   |   0.126384   |   7.88   \n",
            "   3    |  1860   |   0.069727   |   7.87   \n",
            "   3    |  1880   |   0.154471   |   7.88   \n",
            "   3    |  1900   |   0.138540   |   7.88   \n",
            "   3    |  1920   |   0.165948   |   7.88   \n",
            "   3    |  1940   |   0.100589   |   7.89   \n",
            "   3    |  1960   |   0.054530   |   7.88   \n",
            "   3    |  1980   |   0.074693   |   7.88   \n",
            "   3    |  2000   |   0.106799   |   7.87   \n",
            "   3    |  2020   |   0.075465   |   7.88   \n",
            "   3    |  2040   |   0.096686   |   7.88   \n",
            "   3    |  2060   |   0.121289   |   7.85   \n",
            "   3    |  2080   |   0.068531   |   7.84   \n",
            "   3    |  2100   |   0.081393   |   7.83   \n",
            "   3    |  2120   |   0.079668   |   7.86   \n",
            "   3    |  2140   |   0.068658   |   7.83   \n",
            "   3    |  2160   |   0.097631   |   7.84   \n",
            "   3    |  2180   |   0.099820   |   7.86   \n",
            "   3    |  2200   |   0.109170   |   7.83   \n",
            "   3    |  2220   |   0.117004   |   7.84   \n",
            "   3    |  2240   |   0.046570   |   7.86   \n",
            "   3    |  2260   |   0.068175   |   7.85   \n",
            "   3    |  2280   |   0.092888   |   7.84   \n",
            "   3    |  2300   |   0.080967   |   7.83   \n",
            "   3    |  2320   |   0.106917   |   7.83   \n",
            "   3    |  2340   |   0.103162   |   7.82   \n",
            "   3    |  2360   |   0.077498   |   7.83   \n",
            "Average Train Loss: 0.0909949876188566\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    2      |      0.98       |      0.94       |      0.98       |      0.02      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   4    |   20    |   0.050938   |   8.21   \n",
            "   4    |   40    |   0.100798   |   7.84   \n",
            "   4    |   60    |   0.031179   |   7.83   \n",
            "   4    |   80    |   0.119355   |   7.80   \n",
            "   4    |   100   |   0.079939   |   7.81   \n",
            "   4    |   120   |   0.120169   |   7.84   \n",
            "   4    |   140   |   0.081582   |   7.85   \n",
            "   4    |   160   |   0.043744   |   7.86   \n",
            "   4    |   180   |   0.041093   |   7.88   \n",
            "   4    |   200   |   0.093660   |   7.89   \n",
            "   4    |   220   |   0.078486   |   7.88   \n",
            "   4    |   240   |   0.075774   |   7.90   \n",
            "   4    |   260   |   0.069255   |   7.88   \n",
            "   4    |   280   |   0.062305   |   7.87   \n",
            "   4    |   300   |   0.114968   |   7.90   \n",
            "   4    |   320   |   0.105563   |   7.88   \n",
            "   4    |   340   |   0.072337   |   7.87   \n",
            "   4    |   360   |   0.083602   |   7.85   \n",
            "   4    |   380   |   0.062423   |   7.86   \n",
            "   4    |   400   |   0.032496   |   7.85   \n",
            "   4    |   420   |   0.071220   |   7.87   \n",
            "   4    |   440   |   0.037097   |   7.85   \n",
            "   4    |   460   |   0.062633   |   7.86   \n",
            "   4    |   480   |   0.053993   |   7.87   \n",
            "   4    |   500   |   0.113875   |   7.82   \n",
            "   4    |   520   |   0.114050   |   7.88   \n",
            "   4    |   540   |   0.040134   |   7.83   \n",
            "   4    |   560   |   0.078261   |   7.85   \n",
            "   4    |   580   |   0.102566   |   7.86   \n",
            "   4    |   600   |   0.069555   |   7.86   \n",
            "   4    |   620   |   0.097993   |   7.88   \n",
            "   4    |   640   |   0.050513   |   7.86   \n",
            "   4    |   660   |   0.064855   |   7.88   \n",
            "   4    |   680   |   0.025192   |   7.86   \n",
            "   4    |   700   |   0.070753   |   7.86   \n",
            "   4    |   720   |   0.037284   |   7.87   \n",
            "   4    |   740   |   0.075294   |   7.87   \n",
            "   4    |   760   |   0.062735   |   7.89   \n",
            "   4    |   780   |   0.078901   |   7.90   \n",
            "   4    |   800   |   0.038649   |   7.88   \n",
            "   4    |   820   |   0.077984   |   7.88   \n",
            "   4    |   840   |   0.027106   |   7.90   \n",
            "   4    |   860   |   0.079298   |   7.88   \n",
            "   4    |   880   |   0.040987   |   7.86   \n",
            "   4    |   900   |   0.052452   |   7.89   \n",
            "   4    |   920   |   0.076654   |   7.89   \n",
            "   4    |   940   |   0.044134   |   7.87   \n",
            "   4    |   960   |   0.097976   |   7.87   \n",
            "   4    |   980   |   0.067997   |   7.85   \n",
            "   4    |  1000   |   0.054614   |   7.86   \n",
            "   4    |  1020   |   0.014158   |   7.86   \n",
            "   4    |  1040   |   0.098798   |   7.84   \n",
            "   4    |  1060   |   0.082734   |   7.83   \n",
            "   4    |  1080   |   0.118238   |   7.88   \n",
            "   4    |  1100   |   0.126540   |   7.87   \n",
            "   4    |  1120   |   0.183885   |   7.83   \n",
            "   4    |  1140   |   0.201840   |   7.84   \n",
            "   4    |  1160   |   0.085825   |   7.84   \n",
            "   4    |  1180   |   0.208172   |   7.86   \n",
            "   4    |  1200   |   0.133993   |   7.84   \n",
            "   4    |  1220   |   0.083181   |   7.83   \n",
            "   4    |  1240   |   0.095820   |   7.88   \n",
            "   4    |  1260   |   0.148383   |   7.88   \n",
            "   4    |  1280   |   0.064801   |   7.85   \n",
            "   4    |  1300   |   0.060741   |   7.85   \n",
            "   4    |  1320   |   0.055240   |   7.87   \n",
            "   4    |  1340   |   0.102847   |   7.88   \n",
            "   4    |  1360   |   0.027046   |   7.87   \n",
            "   4    |  1380   |   0.076200   |   7.83   \n",
            "   4    |  1400   |   0.053168   |   7.82   \n",
            "   4    |  1420   |   0.054498   |   7.84   \n",
            "   4    |  1440   |   0.070186   |   7.87   \n",
            "   4    |  1460   |   0.075201   |   7.88   \n",
            "   4    |  1480   |   0.082224   |   7.85   \n",
            "   4    |  1500   |   0.069061   |   7.87   \n",
            "   4    |  1520   |   0.066902   |   7.86   \n",
            "   4    |  1540   |   0.132350   |   7.89   \n",
            "   4    |  1560   |   0.088382   |   7.86   \n",
            "   4    |  1580   |   0.089681   |   7.86   \n",
            "   4    |  1600   |   0.026491   |   7.87   \n",
            "   4    |  1620   |   0.081851   |   7.88   \n",
            "   4    |  1640   |   0.053027   |   7.87   \n",
            "   4    |  1660   |   0.044115   |   7.85   \n",
            "   4    |  1680   |   0.039376   |   7.86   \n",
            "   4    |  1700   |   0.083891   |   7.86   \n",
            "   4    |  1720   |   0.010580   |   7.85   \n",
            "   4    |  1740   |   0.071936   |   7.87   \n",
            "   4    |  1760   |   0.043578   |   7.85   \n",
            "   4    |  1780   |   0.061512   |   7.85   \n",
            "   4    |  1800   |   0.108492   |   7.89   \n",
            "   4    |  1820   |   0.054663   |   7.84   \n",
            "   4    |  1840   |   0.053208   |   7.88   \n",
            "   4    |  1860   |   0.066060   |   7.86   \n",
            "   4    |  1880   |   0.024390   |   7.85   \n",
            "   4    |  1900   |   0.038368   |   7.83   \n",
            "   4    |  1920   |   0.067357   |   7.84   \n",
            "   4    |  1940   |   0.111068   |   7.85   \n",
            "   4    |  1960   |   0.037233   |   7.88   \n",
            "   4    |  1980   |   0.111542   |   7.87   \n",
            "   4    |  2000   |   0.163409   |   7.88   \n",
            "   4    |  2020   |   0.131225   |   7.88   \n",
            "   4    |  2040   |   0.081908   |   7.85   \n",
            "   4    |  2060   |   0.090538   |   7.86   \n",
            "   4    |  2080   |   0.159309   |   7.84   \n",
            "   4    |  2100   |   0.129058   |   7.84   \n",
            "   4    |  2120   |   0.111761   |   7.89   \n",
            "   4    |  2140   |   0.213839   |   7.87   \n",
            "   4    |  2160   |   0.166930   |   7.86   \n",
            "   4    |  2180   |   0.184040   |   7.84   \n",
            "   4    |  2200   |   0.055843   |   7.87   \n",
            "   4    |  2220   |   0.105649   |   7.87   \n",
            "   4    |  2240   |   0.069979   |   7.83   \n",
            "   4    |  2260   |   0.087059   |   7.84   \n",
            "   4    |  2280   |   0.083371   |   7.83   \n",
            "   4    |  2300   |   0.039341   |   7.85   \n",
            "   4    |  2320   |   0.127697   |   7.86   \n",
            "   4    |  2340   |   0.181041   |   7.86   \n",
            "   4    |  2360   |   0.070920   |   7.85   \n",
            "Average Train Loss: 0.08198828076705501\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    3      |      0.98       |      0.92       |      0.97       |      0.19      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   5    |   20    |   0.085912   |   8.26   \n",
            "   5    |   40    |   0.153025   |   7.87   \n",
            "   5    |   60    |   0.056092   |   7.84   \n",
            "   5    |   80    |   0.118651   |   7.86   \n",
            "   5    |   100   |   0.065864   |   7.87   \n",
            "   5    |   120   |   0.113644   |   7.85   \n",
            "   5    |   140   |   0.130120   |   7.86   \n",
            "   5    |   160   |   0.104023   |   7.85   \n",
            "   5    |   180   |   0.147417   |   7.85   \n",
            "   5    |   200   |   0.119936   |   7.83   \n",
            "   5    |   220   |   0.107461   |   7.85   \n",
            "   5    |   240   |   0.308229   |   7.83   \n",
            "   5    |   260   |   0.269135   |   7.88   \n",
            "   5    |   280   |   0.262591   |   7.84   \n",
            "   5    |   300   |   0.279942   |   7.84   \n",
            "   5    |   320   |   0.239105   |   7.84   \n",
            "   5    |   340   |   0.218364   |   7.86   \n",
            "   5    |   360   |   0.227069   |   7.86   \n",
            "   5    |   380   |   0.204521   |   7.83   \n",
            "   5    |   400   |   0.246976   |   7.86   \n",
            "   5    |   420   |   0.236637   |   7.87   \n",
            "   5    |   440   |   0.169212   |   7.87   \n",
            "   5    |   460   |   0.146711   |   7.83   \n",
            "   5    |   480   |   0.177692   |   7.85   \n",
            "   5    |   500   |   0.124410   |   7.84   \n",
            "   5    |   520   |   0.159982   |   7.88   \n",
            "   5    |   540   |   0.132068   |   7.86   \n",
            "   5    |   560   |   0.094104   |   7.88   \n",
            "   5    |   580   |   0.044000   |   7.84   \n",
            "   5    |   600   |   0.068230   |   7.85   \n",
            "   5    |   620   |   0.068348   |   7.86   \n",
            "   5    |   640   |   0.087835   |   7.87   \n",
            "   5    |   660   |   0.064565   |   7.87   \n",
            "   5    |   680   |   0.053676   |   7.86   \n",
            "   5    |   700   |   0.048284   |   7.90   \n",
            "   5    |   720   |   0.070237   |   7.87   \n",
            "   5    |   740   |   0.026973   |   7.88   \n",
            "   5    |   760   |   0.079097   |   7.88   \n",
            "   5    |   780   |   0.048304   |   7.89   \n",
            "   5    |   800   |   0.102551   |   7.87   \n",
            "   5    |   820   |   0.030420   |   7.88   \n",
            "   5    |   840   |   0.039057   |   7.88   \n",
            "   5    |   860   |   0.094428   |   7.90   \n",
            "   5    |   880   |   0.108168   |   7.85   \n",
            "   5    |   900   |   0.054448   |   7.86   \n",
            "   5    |   920   |   0.034059   |   7.86   \n",
            "   5    |   940   |   0.131121   |   7.87   \n",
            "   5    |   960   |   0.067336   |   7.86   \n",
            "   5    |   980   |   0.072000   |   7.86   \n",
            "   5    |  1000   |   0.023393   |   7.86   \n",
            "   5    |  1020   |   0.053831   |   7.84   \n",
            "   5    |  1040   |   0.052152   |   7.87   \n",
            "   5    |  1060   |   0.080351   |   7.91   \n",
            "   5    |  1080   |   0.040281   |   7.88   \n",
            "   5    |  1100   |   0.072304   |   7.88   \n",
            "   5    |  1120   |   0.056756   |   7.87   \n",
            "   5    |  1140   |   0.074277   |   7.90   \n",
            "   5    |  1160   |   0.111560   |   7.89   \n",
            "   5    |  1180   |   0.028171   |   7.86   \n",
            "   5    |  1200   |   0.036513   |   7.85   \n",
            "   5    |  1220   |   0.084693   |   7.86   \n",
            "   5    |  1240   |   0.054857   |   7.85   \n",
            "   5    |  1260   |   0.044559   |   7.85   \n",
            "   5    |  1280   |   0.060292   |   7.86   \n",
            "   5    |  1300   |   0.065010   |   7.84   \n",
            "   5    |  1320   |   0.113911   |   7.86   \n",
            "   5    |  1340   |   0.044530   |   7.84   \n",
            "   5    |  1360   |   0.080422   |   7.85   \n",
            "   5    |  1380   |   0.031927   |   7.84   \n",
            "   5    |  1400   |   0.050234   |   7.86   \n",
            "   5    |  1420   |   0.056288   |   7.84   \n",
            "   5    |  1440   |   0.038331   |   7.84   \n",
            "   5    |  1460   |   0.063378   |   7.84   \n",
            "   5    |  1480   |   0.015378   |   7.83   \n",
            "   5    |  1500   |   0.094281   |   7.86   \n",
            "   5    |  1520   |   0.055961   |   7.89   \n",
            "   5    |  1540   |   0.092855   |   7.89   \n",
            "   5    |  1560   |   0.047487   |   7.88   \n",
            "   5    |  1580   |   0.043498   |   7.88   \n",
            "   5    |  1600   |   0.086550   |   7.89   \n",
            "   5    |  1620   |   0.046272   |   7.88   \n",
            "   5    |  1640   |   0.068797   |   7.90   \n",
            "   5    |  1660   |   0.060852   |   7.90   \n",
            "   5    |  1680   |   0.077679   |   7.89   \n",
            "   5    |  1700   |   0.026804   |   7.88   \n",
            "   5    |  1720   |   0.021358   |   7.87   \n",
            "   5    |  1740   |   0.040556   |   7.87   \n",
            "   5    |  1760   |   0.123438   |   7.89   \n",
            "   5    |  1780   |   0.071741   |   7.91   \n",
            "   5    |  1800   |   0.053559   |   7.86   \n",
            "   5    |  1820   |   0.044837   |   7.87   \n",
            "   5    |  1840   |   0.042467   |   7.87   \n",
            "   5    |  1860   |   0.144518   |   7.88   \n",
            "   5    |  1880   |   0.135587   |   7.86   \n",
            "   5    |  1900   |   0.095021   |   7.90   \n",
            "   5    |  1920   |   0.073438   |   7.84   \n",
            "   5    |  1940   |   0.045916   |   7.86   \n",
            "   5    |  1960   |   0.081651   |   7.83   \n",
            "   5    |  1980   |   0.046223   |   7.87   \n",
            "   5    |  2000   |   0.022632   |   7.82   \n",
            "   5    |  2020   |   0.055917   |   7.84   \n",
            "   5    |  2040   |   0.024329   |   7.83   \n",
            "   5    |  2060   |   0.055178   |   7.86   \n",
            "   5    |  2080   |   0.067665   |   7.87   \n",
            "   5    |  2100   |   0.084812   |   7.86   \n",
            "   5    |  2120   |   0.068058   |   7.84   \n",
            "   5    |  2140   |   0.078942   |   7.86   \n",
            "   5    |  2160   |   0.085262   |   7.88   \n",
            "   5    |  2180   |   0.054017   |   7.85   \n",
            "   5    |  2200   |   0.106346   |   7.88   \n",
            "   5    |  2220   |   0.038807   |   7.86   \n",
            "   5    |  2240   |   0.044418   |   7.86   \n",
            "   5    |  2260   |   0.055796   |   7.87   \n",
            "   5    |  2280   |   0.028839   |   7.87   \n",
            "   5    |  2300   |   0.024244   |   7.86   \n",
            "   5    |  2320   |   0.053532   |   7.86   \n",
            "   5    |  2340   |   0.026671   |   7.88   \n",
            "   5    |  2360   |   0.102955   |   7.89   \n",
            "Average Train Loss: 0.08750098223424423\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    4      |      0.98       |      0.96       |      0.98       |      0.66      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   6    |   20    |   0.039824   |   8.29   \n",
            "   6    |   40    |   0.067206   |   7.90   \n",
            "   6    |   60    |   0.007301   |   7.89   \n",
            "   6    |   80    |   0.031707   |   7.85   \n",
            "   6    |   100   |   0.043029   |   7.88   \n",
            "   6    |   120   |   0.064599   |   7.84   \n",
            "   6    |   140   |   0.033304   |   7.87   \n",
            "   6    |   160   |   0.110612   |   7.87   \n",
            "   6    |   180   |   0.041217   |   7.86   \n",
            "   6    |   200   |   0.026491   |   7.87   \n",
            "   6    |   220   |   0.081270   |   7.87   \n",
            "   6    |   240   |   0.037872   |   7.89   \n",
            "   6    |   260   |   0.039361   |   7.87   \n",
            "   6    |   280   |   0.031413   |   7.87   \n",
            "   6    |   300   |   0.029665   |   7.90   \n",
            "   6    |   320   |   0.022881   |   7.88   \n",
            "   6    |   340   |   0.035899   |   7.89   \n",
            "   6    |   360   |   0.036063   |   7.87   \n",
            "   6    |   380   |   0.032323   |   7.88   \n",
            "   6    |   400   |   0.115507   |   7.89   \n",
            "   6    |   420   |   0.028852   |   7.89   \n",
            "   6    |   440   |   0.022685   |   7.88   \n",
            "   6    |   460   |   0.025771   |   7.89   \n",
            "   6    |   480   |   0.077684   |   7.88   \n",
            "   6    |   500   |   0.008209   |   7.88   \n",
            "   6    |   520   |   0.038211   |   7.89   \n",
            "   6    |   540   |   0.091452   |   7.88   \n",
            "   6    |   560   |   0.020993   |   7.88   \n",
            "   6    |   580   |   0.054560   |   7.88   \n",
            "   6    |   600   |   0.073442   |   7.88   \n",
            "   6    |   620   |   0.078230   |   7.86   \n",
            "   6    |   640   |   0.105406   |   7.91   \n",
            "   6    |   660   |   0.224206   |   7.89   \n",
            "   6    |   680   |   0.053539   |   7.88   \n",
            "   6    |   700   |   0.068057   |   7.86   \n",
            "   6    |   720   |   0.108580   |   7.88   \n",
            "   6    |   740   |   0.057600   |   7.88   \n",
            "   6    |   760   |   0.068831   |   7.88   \n",
            "   6    |   780   |   0.050060   |   7.88   \n",
            "   6    |   800   |   0.109125   |   7.88   \n",
            "   6    |   820   |   0.053362   |   7.86   \n",
            "   6    |   840   |   0.039405   |   7.88   \n",
            "   6    |   860   |   0.055398   |   7.87   \n",
            "   6    |   880   |   0.008754   |   7.89   \n",
            "   6    |   900   |   0.038721   |   7.88   \n",
            "   6    |   920   |   0.038397   |   7.85   \n",
            "   6    |   940   |   0.030432   |   7.87   \n",
            "   6    |   960   |   0.037693   |   7.89   \n",
            "   6    |   980   |   0.038588   |   7.89   \n",
            "   6    |  1000   |   0.069033   |   7.88   \n",
            "   6    |  1020   |   0.050789   |   7.88   \n",
            "   6    |  1040   |   0.039210   |   7.89   \n",
            "   6    |  1060   |   0.030941   |   7.88   \n",
            "   6    |  1080   |   0.044123   |   7.89   \n",
            "   6    |  1100   |   0.075346   |   7.89   \n",
            "   6    |  1120   |   0.054786   |   7.91   \n",
            "   6    |  1140   |   0.068782   |   7.87   \n",
            "   6    |  1160   |   0.070071   |   7.90   \n",
            "   6    |  1180   |   0.043935   |   7.90   \n",
            "   6    |  1200   |   0.096955   |   7.89   \n",
            "   6    |  1220   |   0.042254   |   7.89   \n",
            "   6    |  1240   |   0.022025   |   7.88   \n",
            "   6    |  1260   |   0.034379   |   7.88   \n",
            "   6    |  1280   |   0.042433   |   7.88   \n",
            "   6    |  1300   |   0.050657   |   7.89   \n",
            "   6    |  1320   |   0.076389   |   7.86   \n",
            "   6    |  1340   |   0.053602   |   7.87   \n",
            "   6    |  1360   |   0.024002   |   7.88   \n",
            "   6    |  1380   |   0.115690   |   7.89   \n",
            "   6    |  1400   |   0.070133   |   7.87   \n",
            "   6    |  1420   |   0.115045   |   7.87   \n",
            "   6    |  1440   |   0.060815   |   7.88   \n",
            "   6    |  1460   |   0.065630   |   7.86   \n",
            "   6    |  1480   |   0.035620   |   7.87   \n",
            "   6    |  1500   |   0.084056   |   7.85   \n",
            "   6    |  1520   |   0.084330   |   7.86   \n",
            "   6    |  1540   |   0.052248   |   7.86   \n",
            "   6    |  1560   |   0.039616   |   7.88   \n",
            "   6    |  1580   |   0.022752   |   7.85   \n",
            "   6    |  1600   |   0.032667   |   7.85   \n",
            "   6    |  1620   |   0.055061   |   7.88   \n",
            "   6    |  1640   |   0.070046   |   7.84   \n",
            "   6    |  1660   |   0.064800   |   7.87   \n",
            "   6    |  1680   |   0.066111   |   7.87   \n",
            "   6    |  1700   |   0.039592   |   7.86   \n",
            "   6    |  1720   |   0.056481   |   7.87   \n",
            "   6    |  1740   |   0.023093   |   7.88   \n",
            "   6    |  1760   |   0.053452   |   7.88   \n",
            "   6    |  1780   |   0.023128   |   7.86   \n",
            "   6    |  1800   |   0.068466   |   7.85   \n",
            "   6    |  1820   |   0.038351   |   7.86   \n",
            "   6    |  1840   |   0.053012   |   7.88   \n",
            "   6    |  1860   |   0.038132   |   7.88   \n",
            "   6    |  1880   |   0.030090   |   7.88   \n",
            "   6    |  1900   |   0.038470   |   7.90   \n",
            "   6    |  1920   |   0.033833   |   7.89   \n",
            "   6    |  1940   |   0.007078   |   7.87   \n",
            "   6    |  1960   |   0.006696   |   7.86   \n",
            "   6    |  1980   |   0.022356   |   7.88   \n",
            "   6    |  2000   |   0.072834   |   7.87   \n",
            "   6    |  2020   |   0.043746   |   7.88   \n",
            "   6    |  2040   |   0.058508   |   7.89   \n",
            "   6    |  2060   |   0.034387   |   7.88   \n",
            "   6    |  2080   |   0.073980   |   7.89   \n",
            "   6    |  2100   |   0.042242   |   7.87   \n",
            "   6    |  2120   |   0.023240   |   7.86   \n",
            "   6    |  2140   |   0.021789   |   7.87   \n",
            "   6    |  2160   |   0.086953   |   7.85   \n",
            "   6    |  2180   |   0.063211   |   7.88   \n",
            "   6    |  2200   |   0.022499   |   7.86   \n",
            "   6    |  2220   |   0.051808   |   7.88   \n",
            "   6    |  2240   |   0.027411   |   7.89   \n",
            "   6    |  2260   |   0.053887   |   7.88   \n",
            "   6    |  2280   |   0.118960   |   7.88   \n",
            "   6    |  2300   |   0.022872   |   7.87   \n",
            "   6    |  2320   |   0.051947   |   7.86   \n",
            "   6    |  2340   |   0.039088   |   7.88   \n",
            "   6    |  2360   |   0.064638   |   7.88   \n",
            "Average Train Loss: 0.0522349497568839\n",
            "  Epochs   |    Precision    |     Recall      |    Accuracy     |    Threshold   \n",
            "    5      |      0.98       |      0.97       |      0.98       |      0.01      \n",
            " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   7    |   20    |   0.026848   |   8.28   \n",
            "   7    |   40    |   0.065584   |   7.90   \n",
            "   7    |   60    |   0.038583   |   7.89   \n",
            "   7    |   80    |   0.022690   |   7.89   \n",
            "   7    |   100   |   0.007229   |   7.89   \n",
            "   7    |   120   |   0.033919   |   7.86   \n",
            "   7    |   140   |   0.038328   |   7.87   \n",
            "   7    |   160   |   0.054141   |   7.88   \n",
            "   7    |   180   |   0.038511   |   7.84   \n",
            "   7    |   200   |   0.010284   |   7.88   \n",
            "   7    |   220   |   0.006058   |   7.88   \n",
            "   7    |   240   |   0.038527   |   7.86   \n",
            "   7    |   260   |   0.005922   |   7.86   \n",
            "   7    |   280   |   0.041809   |   7.84   \n",
            "   7    |   300   |   0.033886   |   7.83   \n",
            "   7    |   320   |   0.038523   |   7.83   \n",
            "   7    |   340   |   0.054178   |   7.87   \n",
            "   7    |   360   |   0.007158   |   7.88   \n",
            "   7    |   380   |   0.022154   |   7.87   \n",
            "   7    |   400   |   0.066859   |   7.87   \n",
            "   7    |   420   |   0.037718   |   7.88   \n",
            "   7    |   440   |   0.033458   |   7.87   \n",
            "   7    |   460   |   0.022044   |   7.85   \n",
            "   7    |   480   |   0.030953   |   7.85   \n",
            "   7    |   500   |   0.021792   |   7.87   \n",
            "   7    |   520   |   0.018333   |   7.87   \n",
            "   7    |   540   |   0.042931   |   7.87   \n",
            "   7    |   560   |   0.062592   |   7.88   \n",
            "   7    |   580   |   0.005098   |   7.84   \n",
            "   7    |   600   |   0.072290   |   7.87   \n",
            "   7    |   620   |   0.073167   |   7.88   \n",
            "   7    |   640   |   0.015187   |   7.89   \n",
            "   7    |   660   |   0.035744   |   7.87   \n",
            "   7    |   680   |   0.071153   |   7.87   \n",
            "   7    |   700   |   0.043758   |   7.89   \n",
            "   7    |   720   |   0.049584   |   7.88   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pdvUlAVlNYI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}